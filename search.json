[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "Image edited from https://swot.jpl.nasa.gov/resources/87/artists-impression-of-the-swot-satellite/\n\n\nWelcome to the 2022 SWOT Oceanography Cloud Workshop hosted by the Physical Oceanography Distributed Active Archive Center (PO.DAAC), with support provided by the NASA Physical Oceanography Office and NASA Openscapes.\nThe workshop will take place virtually daily March 16 and 17, 2022 from 9am-1pm PST / 12pm-4pm EST (UTC-6). Invited participants will receive a signup link via email."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "About",
    "text": "About\nWorkshop Goal The goal of the workshop is to get ready for Surface Water and Ocean Topography (SWOT) and enable the (oceanography) science team to be ready for processing and handling the large volumes of SWOT SSH data in the cloud. Learning objectives focus on how to access the simulated SWOT L2 SSH data from Earthdata Cloud either by downloading or accessing the data on the cloud. PO.DAAC is the NASA archive for the SWOT mission, and once launched will be making data available via the NASA Earthdata Cloud, hosted in AWS.\nWorkshop Description The workshop has two main components: (1) Pre-event materials will be shared ahead of the workshop and will cover how to access and download cloud-archived simulated SWOT L2 SSH data from the Earthdata Cloud. This material can be found under Logistics - Prerequisites & Homework section of this website. (2) At the workshop, the focus will be on in-could workflows, where no data download is required and data analysis can take place next to the data in the cloud. During the “hacking” time at the workshop participants can choose to employ the “download and analyze locally” or the “in-cloud analysis” paradigm for working with the simulated SWOT L2 SSH data. For the in-cloud workflows, cloud accounts will be provided for the workshop. PO.DAAC will provide necessary tech support during the hands-on “hacking” time.\nWorkshop Outcomes At the end of the two days, participants should be able to access the simulated SWOT L2 SSH data that is archived in the NASA EArthdata Cloud (hosted in AWS) and at minimum, subset and plot a time series of their favorite region, if not more. Participants are encouraged to coordinate amongst their respective SWOT ST project to figure out who will be attending. If the project would like more than one person to attend that is welcomed, just note that as a project you will need to agree on what use case you want to work on. Even if the team members amongst the project are cloud savvy, it is still strongly recommended that there is a representative attending, as this workshop will specifically be covering the SWOT simulated data."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2022 SWOT Oceanography Cloud Workshop is hosted by NASA’s PO.DAAC with support from NASA Physical Oceanography Office and the NASA Openscapes Project, with cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The SWOT Oceanography Cloud Workshop will take place virtually March 16-17, from 9am-1pm PST / 12-4pm EST.\nSee Prerequisites & Homework for pre-workshop preparation.\nWebEx links will be shared directly with this group via a (calendar) meeting invite."
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDay 1 - March 16, 2022\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n9:00 am\nWelcome - Getting settled in\nCatalina Oaida, PO.DAAC Applied Science Systems Engineer\n\n\n9:05 am\nWelcome\nJessica Hausman, NASA Physical Oceanography Program Office\n\n\n9:10 am\nSWOT Status and Challenges\nLee-Lueng Fu, SWOT Project\n\n\n9:15 am\nWorkshop Goals and Intro to the new Earthdata Cloud Paradigm\nCatalina Oaida, PO.DAAC Applied Science Systems Engineer\n\n\n9:35 am\nIntroduce the SWOT L2 simulated dataset\nJinbo Wang, PO.DAAC Project Scientist\n\n\n9:40 am\nEarthdata Search, Downloading (e.g. Subscriber) - Review, Q&A\nSuresh Vannan, PO.DAAC Project Manager\n\n\n10:00 am\nIntro to JupyterHub in AWS (set up netrc)\nLuis Lopez, NSIDC DAAC\n\n\n10:25 am\nTutorial 1: Direct S3 Access for simulated SWOT L2 SSH\nMike Gangl, PO.DAAC Project Systems Engineer\n\n\n11:10 am\nBreak\n\n\n\n11:20 am\nQ&A and Hack-time\nAll\n\n\n12:50 pm\nClosing for the Day, What’s tomorrow\nCatalina Oaida, PO.DAAC\n\n\n1:00 pm\nDay 1 concludes\n\n\n\n\n\nClosing Day 1\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the workshop daily scheduled session, using JupyterHub.\n\nAgenda for tomorrow: what’s coming next.\n\n\n\n\nDay 2 - March 17, 2022\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n9:00 am\nWelcome\nCatalina Oaida, PO.DAAC Applied Science Systems Engineer\n\n\n9:05 am\nTutorial 2: Subsetting simulated SWOT L2 SSH\nCelia Ou/Jack McNelis, PO.DAAC Data Publication Engineers\n\n\n9:35 am\nTutorial 3: Creating simulated SWOT L2 time series\nCelia Ou/Jinbo Wang, PO.DAAC\n\n\n9:50 am\nQ&A / Hack-time\nAll\n\n\n11:45 am\nParticipant Reportout\nParticipants\n\n\n12:45 pm\nWhat’s Next & Q&A/Discussion\nCatalina Oaida, PO.DAAC\n\n\n1:00 pm\nWorkshop Concludes\n\n\n\n\n\nWelcome Day 2\n\nJupyterHub: Log in.\n\nLog into 2i2c. This takes a few minutes so please start this as soon we reconvene each day.\n\nGitHub: Get the latest.\n\nYou may need to git fetch, merge, and pull: follow the github workflows daily setup.\n\n\n\n\nClosing Day 2\n\nThank you!\nContinued hacking on the cloud - next 3 months. You will continue to have access to the 2i2c JupyterHub in AWS for three months following the SWOT Ocean Cloud Workshop so you can continue to work and we all learn more about what is involved with migrating data access and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project."
  },
  {
    "objectID": "schedule.html#getting-help-during-the-workshop",
    "href": "schedule.html#getting-help-during-the-workshop",
    "title": "Schedule",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nWe will use WebEx Chat as our main channels for help. Please use WebEx Chat to post questions, or request a breakout room."
  },
  {
    "objectID": "further-resources.html",
    "href": "further-resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "One stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "further-resources.html#additional-tutorials",
    "href": "further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  },
  {
    "objectID": "Direct_Access_netCDF_simple.html",
    "href": "Direct_Access_netCDF_simple.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\n\n\n\n\nimport needed libraries\ndefine dataset of interest\nauthenticate for NASA Earthdata archive (Earthdata Login)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\n\nimport os\nimport subprocess\nfrom os.path import dirname, join\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport boto3\nimport s3fs\n\n# Read and work with datasets\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\n\n\n\nIn this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\n\n\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\nSee the following (Authentication for NASA Earthdata tutorial)[https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html]\n\n\n\nPass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets. For now, each DAAC has different AWS credentials endpoints. LP DAAC and PO.DAAC are listed here:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n}\n\nIn this example we’re interested in the ECCO data collection from PO.DAAC in Earthdata Cloud in AWS S3, so we specify the podaac endpoint in the next code block.\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=s3_cred_endpoint['podaac']):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\n\n\nNow that we have authenticated in AWS, this next code block accesses data directly from the NASA Earthdata archive in an S3 bucket in us-west-2 region, without downloading or moving any files into your user cloud workspace (instnace).\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\n\n\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<cartopy.mpl.geocollection.GeoQuadMesh at 0x7fd040602b20>\n\n\n\n\n\n\n\n\n\nFull example with additional plots and use cases here: https://github.com/podaac/ECCO/blob/main/Data_Access/cloud_direct_access_s3.ipynb"
  },
  {
    "objectID": "prerequisites/index.html",
    "href": "prerequisites/index.html",
    "title": "Prerequisites & Homework",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nThe workshop signup form asks for your GitHub username - this allows us to enable you access to a cloud environment during the workshop.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You can follow along in Jupyter Hub on your own computer while also watching an instructor demo over WebEx (or equivalent)."
  },
  {
    "objectID": "prerequisites/index.html#pre-workshop-homework",
    "href": "prerequisites/index.html#pre-workshop-homework",
    "title": "Prerequisites & Homework",
    "section": "Pre-Workshop Homework",
    "text": "Pre-Workshop Homework\nWe recommmend watching or walking through the following tutorials ahead of the workshop, to become familiar with topics that we won’t have time to take a deep dive into during the workshop, but which are important pieces in accessing (simulated) SWOT data hosted in the NASA Earthdata Cloud. You will get the chance to ask any questions regarding the material covered in these pre-workshop tutorials during the workshop.\n\nTutorial: Search for PO.DAAC data using the Earthdata Search GUI (15 min)\n\nGeneric tutorial on how to search and retrieve download or access links for PO.DAAC data using the Earthdata Search GUI.\n\nTutorial: NASA Earthdata Authentication (5 minutes)\n\nTutorial for generating a netrc file, which enables programmatic (command line) NASA Earthdata Authentication. If you don’t have a NASA Earthdata user login account, you can set up a free one quickly. See the Prerequisites section above.\n\nTutorial: Download data from the NASA Earthdata Cloud using the PO.DAAC data-subscriber (12 minutes)\n\nWhile the workshop will focus on data access within the cloud, data download from the NASA Earthdata Cloud is still possbile. This tutorial will walk you through how to download simulated SWOT L2 data from the PO.DAAC archive in Earthdata Cloud (hosted in AWS), using the PO.DAAC data-subscriber tool."
  },
  {
    "objectID": "prerequisites/01_Earthdata_Search.html",
    "href": "prerequisites/01_Earthdata_Search.html",
    "title": "01. Earthdata Search",
    "section": "",
    "text": "Step 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 104 matching collections were found with the basic ECCO search.\n\n\n\nFigure caption: Search for ECCO data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Let’s search for ECCO monthly SSH in the search box (which will produce 39 matching collections), and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box.\nScroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECCO_L4_SSH_05DEG_MONTHLY_V4R4.\n\n\n\nFigure caption: Refine search, set temporal bounds, get more information\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\nthe S3 storage bucket and object prefix where this data is located\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nPro Tip: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more. CMR API and CMR STAC API tutorials can be found on the 2021 Cloud Hackathon website.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (here again it’s the same ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4. Customize the download or data access\nClick on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\n\n4.a. Entire file content\nLet’s stay we are interested in the entire file content, so we select the “Direct Download” option (as opposed to other options to subset or transform the data):\n\n\n\nFigure caption: Customize your download or access\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n  \nThe Download Files tab provides the https:// links for downloading the files locally. E.g.: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud (an example will be shown in Tutorial 3). E.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\nTip: Another quicker way to find the bucket and object prefix is from the list of data files the search returns. Next to the + green button is a grey donwload symbol. Click on that to see the Download Files https:// links or on the AWS S3 Access to get the direct S3:// access links, which contain the bucket and object prefix where data is stored.\n\n\n4.b. Subset or transform before download or access\nDAAC tools and services are also being migrated or developed in the cloud, next to that data. These include the Harmony API and OPeNDAP in the cloud, as a few examples.\nWe can leverage these cloud-based services on cloud-archived data to reduce or transform the data (depending on need) before getting the access links regardless of whether we prefer to download the data and work on a local machine or whether we want to access the data in the cloud (from a cloud workspace). These can be useful data reduction services that support a faster time to science.\nHarmony\nHarmony allows you to seamlessly analyze Earth observation data from different NASA data centers. These services (API endpoints) provide data reduction (e.g. subsetting) and transfromation services (e.g. convert netCDF data to Zarr cloud optimized format).\n\n\n\nFigure caption: Leverage Harmony cloud-based data transformation services\n\n\nWhen you click the final green Download button, the links provided are to data that had been transformed based on our selections on the previous screen (here chosing to use the Harmony service to reformat the data to Zarr). These data are staged for us in an S3 bucket in AWS, and we can use the s3:// links to access those specific data. This service also provides STAC access links. This particular example is applicable if your workflow is in the AWS us-west-2 region.\n\n\n\nFigure caption: Harmony-staged data in S3\n\n\n\n\n\nStep 5. Integrate file links into programmatic workflow, locally or in the AWS cloud.\nIn tutorial 3 Direct Data Access, we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\nTutorial 3 will pick up from here and cover these next steps in more detail."
  },
  {
    "objectID": "prerequisites/03_download.html",
    "href": "prerequisites/03_download.html",
    "title": "03. Download with PO.DAAC data-subscriber",
    "section": "",
    "text": "While the workshop will focus on data access within the cloud, data download from the NASA Earthdata Cloud is still possbile. This tutorial will walk you through how to download simulated SWOT L2 data from the PO.DAAC archive in Earthdata Cloud (hosted in AWS), using the PO.DAAC data-subscriber tool."
  },
  {
    "objectID": "prerequisites/02_NASA_Earthdata_Authentication.html",
    "href": "prerequisites/02_NASA_Earthdata_Authentication.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively.\n\n\n\n\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\n\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "Tutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/podaac/2022-SWOT-Ocean-Cloud-Workshop/tree/main/tutorials."
  },
  {
    "objectID": "tutorials/Finding_collection_s3_location.html",
    "href": "tutorials/Finding_collection_s3_location.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "We need to determine the path for our products of interest. We can do this through several mechanisms.\n\n\nThe easiest of which is through the PO.DAAC Cloud Dataset Listing page: https://podaac.jpl.nasa.gov/cloud-datasets\n\n\n\nS3 Data Locations from Portal\n\n\nFor each dataset, the ‘Data Access’ tab will have various information, but will always contain the S3 paths listed specifically. Data files will always be found under the ‘protected’ bucket.\n\n\n\nFrom the Earthdata Search Client (search.earthdata.nasa.gov), collection level information can be found by clicking the ‘i’ on a collection search result. An example of this is seen below:\n\n\n\nS3 Data Locations from Search 1\n\n\nOnce on the collection inforamtion screen, the S3 bucket locations can be found by scrolling to the bottom of the information panel. The SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1 example is shown below.\n\n\n\nS3 Data Locations from Search 2\n\n\n\n\n\nOne can query the collection identifier to get information from CMR:\nhttps://cmr.earthdata.nasa.gov/search/concepts/C2152045877-POCLOUD.umm_json\nThe identifier is found on the PO.DAAC Cloud Data Set Listing page entries, called ‘Collection Concept ID’\nResults returned will look like the following:\n{\n    ...\n    \"DirectDistributionInformation\": {\n        \"Region\": \"us-west-2\",\n        \"S3BucketAndObjectPrefixNames\": [\n            \"podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\",\n            \"podaac-ops-cumulus-public/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\"\n        ],\n        \"S3CredentialsAPIEndpoint\": \"https://archive.podaac.earthdata.nasa.gov/s3credentials\",\n        \"S3CredentialsAPIDocumentationURL\": \"https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME\"\n    },\n    ...\n}"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html",
    "href": "tutorials/SWOT_sim_data_intro.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "Jinbo Wang\nSWOT Scientist/PODAAC Project Scientist\n3/16/2022"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#swot-oceanography-datacode-survey-2021-science-team-meeting",
    "href": "tutorials/SWOT_sim_data_intro.html#swot-oceanography-datacode-survey-2021-science-team-meeting",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "SWOT Oceanography data/code survey (2021 Science Team Meeting)",
    "text": "SWOT Oceanography data/code survey (2021 Science Team Meeting)\nShortly before the 2021 SWOT Science Team meeting, a survey was created to ask for the SWOT oceanography community’s opinion about data and code sharing. In summary, the community showed high interests in a set of synthetic SWOT global L2 SSH hosted by PODAAC/AVISO, as well as pre-launch training. The survey response can be accessed here. The following are a few takeaways.\n\n\n\nimage.png\n\n\n\nL2 basic/expert SSH will be mostly used data products at the initial stage\nA common datesets will be very useful for preparation of the SWOT research\nIt is important for PODAAC/AVISO to host simulated datasets to create postlaunch scenarios\nMost teams need training in data access and analyses\nMajorities are potentially interested in co-developing softwares and toolboxes."
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#simulated-l2-ssh",
    "href": "tutorials/SWOT_sim_data_intro.html#simulated-l2-ssh",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Simulated L2 SSH",
    "text": "Simulated L2 SSH\nThe SWOTsimulator was used on two global ocean simulations (LLC4320 and GLORYS) following the error specification described in Level 2 KaRIn Low Rate Sea Surface Height Product PDF file (D-56407). The simulator was based on Lucile/Clement/Fu’s first version but was almost completely rewritten, while keeping the error budget largely unchanged. (The error representation is claimed to be better but needs more documentation.) The CNES team is the creator of these products. The 8 datasets are listed as follows:\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1    10288 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_CALVAL_V1    10287 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nThe links to these datasets on PODAAC website or Earthdata search page.\n\nEight simulated L2 SSH datasets on PO.DAAC website.\nFour GLORYS datasets from Earthdata search\nFour ECCO_LLC4320 datasets from Earthdata search\n\nThe ECCO_LLC4320-based products cover one-year duration with >10k files (granules in Earthdata language) in each collection. The GLORYS-based products cover about 20 months with >17k files in each collection."
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#relevant-content-in-the-simulated-l2-ssh-files",
    "href": "tutorials/SWOT_sim_data_intro.html#relevant-content-in-the-simulated-l2-ssh-files",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Relevant content in the simulated L2 SSH files",
    "text": "Relevant content in the simulated L2 SSH files\nThere are too much information in these datasets for this short cloud hackathon. For example, there are 92 variables in the KaRIn products. The hackathon participants are strongly encouraged to read the the product description to understand the meaning of different variables. For this exercise, the variables with “simulated” keyword are most relevant: 1. ‘simulated_true_ssh_karin’ 1. ‘simulated_error_baseline_dilation’ 1. ‘simulated_error_roll’ 1. ‘simulated_error_phase’ 1. ‘simulated_error_timing’ 1. ‘simulated_error_karin’ 1. ‘simulated_error_orbital’ 1. ‘simulated_error_troposphere’\nA visualized example is provided at the end of this notebook.\nNote that the LLC4320 SSH includes the barotropic (BT) signals, internal tides and atmospheric pressure (IB). A proper correction is not yet provided/validated. Wang et al. (2018) found that a linear detrend within ~150km range may be sufficient to remove most of the large-scale BT and IB signals. It can be a quick solution but no guarantee about the size of the residual errors."
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#other-swot-relevant-datasets",
    "href": "tutorials/SWOT_sim_data_intro.html#other-swot-relevant-datasets",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Other SWOT relevant datasets",
    "text": "Other SWOT relevant datasets\nOther datasets in PODAAC that are relevant to SWOT include 10 regional subsets of LLC4320 with all the 2D and 3D fields created to support Adopt-A-Crossover (AdAC) project. You will find the dynamic correspondence of the ECCO_LLC4320-based L2 SSH to these subsets. Sentinel-6MF alongtrack data are also hosted in PODAAC cloud.\n\nAll LLC4320-derived datasets in PODAAC\nSentinel-6MF alongtracks"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#acknowledgment",
    "href": "tutorials/SWOT_sim_data_intro.html#acknowledgment",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Acknowledgment",
    "text": "Acknowledgment\n\nProject/HQ\n\nF. Briol, Gerald Dibarbource, Nicolas Picot, Shailen Desai produced the data products\nJ. Tom Farrar, Julien le Sommer, Ryan Abernathey, Sarah Gille, Rosemary Morrow, Lee-Lueng Fu provided science team support\nNadya Vinogradova requested this cloud hackathon. Jessica Hausman facilitated the coordination.\nJustin Rice facilitates the PODAAC-openscapes collaboration\n\nOpenscapes provides cloud-jupyterhub\nPO.DAAC team (everything else here)"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#browsing-the-8-datasets-in-s3podaac-ops-cumulus-protected",
    "href": "tutorials/SWOT_sim_data_intro.html#browsing-the-8-datasets-in-s3podaac-ops-cumulus-protected",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Browsing the 8 datasets in s3://podaac-ops-cumulus-protected",
    "text": "Browsing the 8 datasets in s3://podaac-ops-cumulus-protected\nAll PODAAC data collections are stored in s3://podaac-ops-cumulus-protected and/or s3://podaac-ops-cumulus-public. Once you are on AWS computing instance, they can be accessed like accessing to your own harddrives from your laptop.\n\n#The 8 datasets have short names starting from \"SWOT\". \n#You may get more than these 8 collections in the future after new SWOT data ingested. \n#The following wildcard will give you the 8 collections for now.\ns3path=\"s3://podaac-ops-cumulus-protected/SWOT*\"  \n\n#Search all collections that fit to the wildcard.\nfns= s3sys.glob(s3path)\n\n#Print the short names of the collection (also used as 'folder' names) and the number of files (granules) within. \nfor aa in fns:\n    print('%55s'%aa.split('/')[-1], len(s3sys.glob(aa+'/*nc')), 'files')\n\n#Print the name of the first 10 files in the SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1 collection.\nfns=s3sys.glob(\"s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/*nc\")\npprint(fns[:10])\n\n     SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1 10288 files\n    SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1 10218 files\n           SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1 17686 files\n          SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1 17564 files\n     SWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_CALVAL_V1 10287 files\n    SWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_SCIENCE_V1 10218 files\n           SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_CALVAL_V1 17686 files\n          SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1 17564 files\n['podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_001_20111113T000000_20111113T005105_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_002_20111113T005105_20111113T014211_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_003_20111113T014211_20111113T023317_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_004_20111113T023317_20111113T032423_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_005_20111113T032423_20111113T041529_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_006_20111113T041529_20111113T050634_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_007_20111113T050634_20111113T055739_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_008_20111113T055739_20111113T064845_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_009_20111113T064845_20111113T073951_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/SWOT_L2_LR_SSH_Expert_001_010_20111113T073951_20111113T083057_DG10_01.nc']"
  },
  {
    "objectID": "tutorials/SWOT_sim_data_intro.html#print-the-content-of-an-example-file",
    "href": "tutorials/SWOT_sim_data_intro.html#print-the-content-of-an-example-file",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Print the content of an example file",
    "text": "Print the content of an example file\nThe following code uses the function temporarily defined in the first block to open a file from ‘fns’. The function includes establishing a direct S3 access and opening with xarray. It returns a xarray Dataset.\n\ndata=open_swot_L2SSH(fns[0])\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                                (num_lines: 9868, num_pixels: 71,\n                                            num_sides: 2)\nCoordinates:\n    latitude                               (num_lines, num_pixels) float64 ...\n    longitude                              (num_lines, num_pixels) float64 ...\n    latitude_nadir                         (num_lines) float64 -77.66 ... 77.66\n    longitude_nadir                        (num_lines) float64 144.8 ... 311.9\nDimensions without coordinates: num_lines, num_pixels, num_sides\nData variables: (12/92)\n    time                                   (num_lines) datetime64[ns] 2011-11...\n    time_tai                               (num_lines) datetime64[ns] 2011-11...\n    ssh_karin                              (num_lines, num_pixels) float64 ...\n    ssh_karin_uncert                       (num_lines, num_pixels) float32 ...\n    ssha_karin                             (num_lines, num_pixels) float64 ...\n    ssh_karin_2                            (num_lines, num_pixels) float64 ...\n    ...                                     ...\n    simulated_error_timing                 (num_lines, num_pixels) float64 ...\n    simulated_error_roll                   (num_lines, num_pixels) float64 ...\n    simulated_error_phase                  (num_lines, num_pixels) float64 ...\n    simulated_error_karin                  (num_lines, num_pixels) float64 ...\n    simulated_error_orbital                (num_lines, num_pixels) float64 ...\n    simulated_error_troposphere            (num_lines, num_pixels) float64 ...\nAttributes: (12/32)\n    Conventions:                CF-1.7\n    title:                      Level 2 Low Rate Sea Surface Height Data Prod...\n    institution:                CNES/JPL\n    source:                     Simulate product\n    history:                    2021-09-23 07:47:11Z : Creation\n    platform:                   SWOT\n    ...                         ...\n    right_last_longitude:       311.89957599684425\n    right_last_latitude:        77.03365811434979\n    wavelength:                 0.008385803020979\n    orbit_solution:             POE\n    ellipsoid_semi_major_axis:  6378137.0\n    ellipsoid_flattening:       0.0033528106647474805xarray.DatasetDimensions:num_lines: 9868num_pixels: 71num_sides: 2Coordinates: (4)latitude(num_lines, num_pixels)float64...long_name :latitude (positive N, negative S)standard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Latitude of measurement [-80,80]. Positive latitude is North latitude, negative latitude is South latitude.[700628 values with dtype=float64]longitude(num_lines, num_pixels)float64...long_name :longitude (degrees East)standard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude of measurement. East longitude relative to Greenwich meridian.[700628 values with dtype=float64]latitude_nadir(num_lines)float64...long_name :latitude of satellite nadir pointstandard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Geodetic latitude [-80,80] (degrees north of equator) of the satellite nadir point.array([-77.662495, -77.662454, -77.662388, ...,  77.663153,  77.663181,\n        77.663183])longitude_nadir(num_lines)float64...long_name :longitude of satellite nadir pointstandard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude (degrees east of Grenwich meridian) of the satellite nadir point.array([144.766698, 144.850876, 144.935054, ..., 311.732881, 311.817064,\n       311.901247])Data variables: (92)time(num_lines)datetime64[ns]...long_name :time in UTCstandard_name :timeleap_second :YYYY-MM-DDThh:mm:ssZcomment :Time of measurement in seconds in the UTC time scale since 1 Jan 2000 00:00:00 UTC. [tai_utc_difference] is the difference between TAI and UTC reference time (seconds) for the first measurement of the data set. If a leap second occurs within the data set, the attribute leap_second is set to the UTC time at which the leap second occurs.tai_utc_difference :34.0array(['2011-11-16T13:09:22.641982976', '2011-11-16T13:09:22.955580992',\n       '2011-11-16T13:09:23.269179968', ..., '2011-11-16T14:00:27.610507008',\n       '2011-11-16T14:00:27.922805952', '2011-11-16T14:00:28.235110016'],\n      dtype='datetime64[ns]')time_tai(num_lines)datetime64[ns]...long_name :time in TAIstandard_name :timetai_utc_difference :[Value of TAI-UTC at time of first record]comment :Time of measurement in seconds in the TAI time scale since 1 Jan 2000 00:00:00 TAI. This time scale contains no leap seconds. The difference (in seconds) with time in UTC is given by the attribute [time:tai_utc_difference].array(['2011-11-16T13:09:56.641982976', '2011-11-16T13:09:56.955580992',\n       '2011-11-16T13:09:57.269179968', ..., '2011-11-16T14:01:01.610507008',\n       '2011-11-16T14:01:01.922805952', '2011-11-16T14:01:02.235110016'],\n      dtype='datetime64[ns]')ssh_karin(num_lines, num_pixels)float64...long_name :sea surface heightstandard_name :sea surface height above reference ellipsoidunits :mvalid_min :-15000000valid_max :150000000comment :Fully corrected sea surface height measured by KaRIn. The height is relative to the reference ellipsoid defined in the global attributes. This value is computed using radiometer measurements for wet troposphere effects on the KaRIn measurement (e.g., rad_wet_tropo_cor and sea_state_bias_cor).[700628 values with dtype=float64]ssh_karin_uncert(num_lines, num_pixels)float32...long_name :sea surface height anomaly uncertaintyunits :mvalid_min :0valid_max :60000comment :1-sigma uncertainty on the sea surface height from the KaRIn measurement.[700628 values with dtype=float32]ssha_karin(num_lines, num_pixels)float64...long_name :sea surface height anomalyunits :mvalid_min :-1000000valid_max :1000000comment :Sea surface height anomaly from the KaRIn measurement = ssh_karin - mean_sea_surface_cnescls - solid_earth_tide - ocean_tide_fes – internal_tide_hret - pole_tide - dac.[700628 values with dtype=float64]ssh_karin_2(num_lines, num_pixels)float64...long_name :sea surface heightstandard_name :sea surface height above reference ellipsoidunits :mvalid_min :-15000000valid_max :150000000comment :Fully corrected sea surface height measured by KaRIn. The height is relative to the reference ellipsoid defined in the global attributes. This value is computed using model-based estimates for wet troposphere effects on the KaRIn measurement (e.g., model_wet_tropo_cor and sea_state_bias_cor_2).[700628 values with dtype=float64]ssha_karin_2(num_lines, num_pixels)float64...long_name :sea surface height anomalyunits :mvalid_min :-1000000valid_max :1000000comment :Sea surface height anomaly from the KaRIn measurement = ssh_karin_2 - mean_sea_surface_cnescls - solid_earth_tide - ocean_tide_fes – internal_tide_hret - pole_tide - dac.[700628 values with dtype=float64]ssha_karin_qual(num_lines, num_pixels)float64...long_name :sea surface height quality flagstandard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for the SSHA from KaRIn.[700628 values with dtype=float64]polarization_karin(num_lines, num_sides)object...long_name :polarization for each side of the KaRIn swathcomment :H denotes co-polarized linear horizontal, V denotes co-polarized linear vertical.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=object)swh_karin(num_lines, num_pixels)float32...long_name :significant wave height from KaRInstandard_name :sea_surface_wave_significant_heightunits :mvalid_min :0valid_max :25000comment :Significant wave height from KaRIn volumetric correlation.[700628 values with dtype=float32]swh_karin_uncert(num_lines, num_pixels)float32...long_name :1-sigma uncertainty on significant wave height from KaRInunits :mvalid_min :0valid_max :25000comment :1-sigma uncertainty on significant wave height from KaRIn.[700628 values with dtype=float32]sig0_karin(num_lines, num_pixels)float32...long_name :normalized radar cross section (sigma0) from KaRInstandard_name :surface_backwards_scattering_coefficient_of_radar_waveunits :1valid_min :-1000.0valid_max :10000000.0comment :Normalized radar cross section (sigma0) from KaRIn in real, linear units (not decibels).  The value may be negative due to noise subtraction.  The value is corrected for instrument calibration and atmospheric attenuation. Radiometer measurements provide the atmospheric attenuation (sig0_cor_atmos_rad).[700628 values with dtype=float32]sig0_karin_uncert(num_lines, num_pixels)float32...long_name :1-sigma uncertainty on sigma0 from KaRInunits :1valid_min :0.0valid_max :1000.0comment :1-sigma uncertainty on sigma0 from KaRIn.[700628 values with dtype=float32]sig0_karin_2(num_lines, num_pixels)float32...long_name :normalized radar cross section (sigma0) from KaRInstandard_name :surface_backwards_scattering_coefficient_of_radar_waveunits :1valid_min :-1000.0valid_max :10000000.0comment :Normalized radar cross section (sigma0) from KaRIn in real, linear units (not decibels).  The value may be negative due to noise subtraction.  The value is corrected for instrument calibration and atmospheric attenuation. A meteorological model provides the atmospheric attenuation (sig0_cor_atmos_model).[700628 values with dtype=float32]wind_speed_karin(num_lines, num_pixels)float32...long_name :wind speed from KaRIn standard_name :wind_speedsource :TBDunits :m/svalid_min :0valid_max :65000comment :Wind speed from KaRIn computed from sig0_karin.[700628 values with dtype=float32]wind_speed_karin_2(num_lines, num_pixels)float32...long_name :wind speed from KaRIn standard_name :wind_speedsource :TBDunits :m/svalid_min :0valid_max :65000comment :Wind speed from KaRIn computed from sig0_karin_2.[700628 values with dtype=float32]swh_karin_qual(num_lines, num_pixels)float64...long_name :quality flag for significant wave height from KaRIn.standard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for significant wave height from KaRIn.[700628 values with dtype=float64]sig0_karin_qual(num_lines, num_pixels)float64...long_name :quality flag for sigma0 from KaRIn.standard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for sigma0 from KaRIn.[700628 values with dtype=float64]num_pt_avg(num_lines, num_pixels)float32...long_name :number of samples averagedunits :1valid_min :0valid_max :289comment :Number of native unsmoothed, beam-combined KaRIn samples averaged.[700628 values with dtype=float32]swh_model(num_lines, num_pixels)float32...long_name :significant wave height from wave modelstandard_name :sea_surface_wave_significant_heightsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mvalid_min :0valid_max :30000comment :Significant wave height from model.[700628 values with dtype=float32]mean_wave_direction(num_lines, num_pixels)float32...long_name :mean sea surface wave directionsource :Meteo France Wave Model (MF-WAM)institution :Meteo Franceunits :degreevalid_min :0valid_max :36000comment :Mean sea surface wave direction.[700628 values with dtype=float32]mean_wave_period_t02(num_lines, num_pixels)float32...long_name :sea surface wind wave mean periodstandard_name :sea_surface_wave_significant_periodsource :Meteo France Wave Model (MF-WAM)institution :Meteo Franceunits :svalid_min :0valid_max :100comment :Sea surface wind wave mean period from model spectral density second moment.[700628 values with dtype=float32]wind_speed_model_u(num_lines, num_pixels)float32...long_name :u component of model windstandard_name :eastward_windsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :m/svalid_min :-30000valid_max :30000comment :Eastward component of the atmospheric model wind vector at 10 meters.[700628 values with dtype=float32]wind_speed_model_v(num_lines, num_pixels)float32...long_name :v component of model windstandard_name :northward_windsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :m/svalid_min :-30000valid_max :30000comment :Northward component of the atmospheric model wind vector at 10 meters.[700628 values with dtype=float32]wind_speed_rad(num_lines, num_sides)float32...long_name :wind speed from radiometerstandard_name :wind_speedsource :Advanced Microwave Radiometerunits :m/svalid_min :0valid_max :65000comment :Wind speed from radiometer measurements.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)distance_to_coast(num_lines, num_pixels)float32...long_name :distance to coastsource :MODIS/GlobCoverinstitution :European Space Agencyunits :mvalid_min :0valid_max :21000comment :Approximate distance to the nearest coast point along the Earth surface.[700628 values with dtype=float32]heading_to_coast(num_lines, num_pixels)float32...long_name :heading to coastunits :degreesvalid_min :0valid_max :35999comment :Approximate compass heading (0-360 degrees with respect to true north) to the nearest coast point.[700628 values with dtype=float32]ancillary_surface_classification_flag(num_lines, num_pixels)float32...long_name :surface classificationstandard_name :status_flagsource :MODIS/GlobCoverinstitution :European Space Agencyflag_meanings :open_ocean land continental_water aquatic_vegetation continental_ice_snow floating_ice salted_basinflag_values :[0 1 2 3 4 5 6]valid_min :0valid_max :6comment :7-state surface type classification computed from a mask built with MODIS and GlobCover data.[700628 values with dtype=float32]dynamic_ice_flag(num_lines, num_pixels)float32...long_name :dynamic ice flagstandard_name :status_flagsource :EUMETSAT Ocean and Sea Ice Satellite Applications Facilityinstitution :EUMETSATflag_meanings :no_ice probable_ice iceflag_values :[0 1 2]valid_min :0valid_max :2comment :Dynamic ice flag for the location of the KaRIn measurement.[700628 values with dtype=float32]rain_flag(num_lines, num_pixels)float32...long_name :rain flagstandard_name :status_flagflag_meanings :no_rain probable_rain rainflag_values :[0 1 2]valid_min :0valid_max :2comment :Flag indicates that signal is attenuated, probably from rain.[700628 values with dtype=float32]rad_surface_type_flag(num_lines, num_sides)float32...long_name :radiometer surface type flagstandard_name :status_flagsource :Advanced Microwave Radiometerflag_meanings :open_ocean coastal_ocean landflag_values :[0 1 2]valid_min :0valid_max :2comment :Flag indicating the validity and type of processing applied to generate the wet troposphere correction (rad_wet_tropo_cor). A value of 0 indicates that open ocean processing is used, a value of 1 indicates coastal processing, and a value of 2 indicates that rad_wet_tropo_cor is invalid due to land contamination.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)sc_altitude(num_lines)float64...long_name :altitude of KMSF originstandard_name :height_above_reference_ellipsoidunits :mvalid_min :0valid_max :2000000000comment :Altitude of the KMSF origin.array([nan, nan, nan, ..., nan, nan, nan])orbit_alt_rate(num_lines)float32...long_name :orbital altitude rate with respect to mean sea surfaceunits :m/svalid_min :-3500valid_max :3500comment :Orbital altitude rate with respect to the mean sea surface.array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)cross_track_angle(num_lines)float64...long_name :cross-track angle from true northunits :degreesvalid_min :0valid_max :359999999comment :Angle with respect to true north of the cross-track direction to the right of the spacecraft velocity vector.array([nan, nan, nan, ..., nan, nan, nan])sc_roll(num_lines)float64...long_name :roll of the spacecraftstandard_name :platform_roll_angleunits :degreesvalid_min :-1799999valid_max :1800000comment :KMSF attitude roll angle; positive values move the +y antenna down.array([nan, nan, nan, ..., nan, nan, nan])sc_pitch(num_lines)float64...long_name :pitch of the spacecraftstandard_name :platform_pitch_angleunits :degreesvalid_min :-1799999valid_max :1800000comment :KMSF attitude pitch angle; positive values move the KMSF +x axis up.array([nan, nan, nan, ..., nan, nan, nan])sc_yaw(num_lines)float64...long_name :yaw of the spacecraftstandard_name :platform_yaw_angleunits :degreesvalid_min :-1799999valid_max :1800000comment :KMSF attitude yaw angle relative to the nadir track. The yaw angle is a right-handed rotation about the nadir (downward) direction. A yaw value of 0 deg indicates that the KMSF +x axis is aligned with the horizontal component of the Earth-relative velocity vector. A yaw value of 180 deg indicates that the spacecraft is in a yaw-flipped state, with the KMSF -x axis aligned with the horizontal component of the Earth-relative velocity vector.array([nan, nan, nan, ..., nan, nan, nan])velocity_heading(num_lines)float64...long_name :heading of the spacecraft Earth-relative velocity vectorunits :degreesvalid_min :0valid_max :359999999comment :Angle with respect to true north of the horizontal component of the spacecraft Earth-relative velocity vector. A value of 90 deg indicates that the spacecraft velocity vector pointed due east. Values between 0 and 90 deg indicate that the velocity vector has a northward component, and values between 90 and 180 deg indicate that the velocity vector has a southward component.array([nan, nan, nan, ..., nan, nan, nan])orbit_qual(num_lines)float32...long_name :orbit quality flag standard_name :status_flagvalid_min :0valid_max :1comment :Orbit quality flag.array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)latitude_avg_ssh(num_lines, num_pixels)float64...long_name :weighted average latitude of samples used to compute SSHstandard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Latitude of measurement [-80,80]. Positive latitude is North latitude, negative latitude is South latitude.  This value may be biased away from a nominal grid location if some of the native, unsmoothed samples were discarded during processing.[700628 values with dtype=float64]longitude_avg_ssh(num_lines, num_pixels)float64...long_name :weighted average longitude of samples used to compute SSHstandard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude of measurement. East longitude relative to Greenwich meridian.  This value may be biased away from a nominal grid location if some of the native, unsmoothed samples were discarded during processing.[700628 values with dtype=float64]cross_track_distance(num_lines, num_pixels)float32...long_name :cross track distanceunits :mvalid_min :-75000.0valid_max :75000.0comment :Distance of sample from nadir. Negative values indicate the left side of the swath, and positive values indicate the right side of the swath.[700628 values with dtype=float32]x_factor(num_lines, num_pixels)float32...long_name :radiometric calibration X factor as a composite value for the X factors of the +y and -y channelsunits :1valid_min :0.0valid_max :1e+20comment :Radiometric calibration X factor as a linear power ratio.[700628 values with dtype=float32]sig0_cor_atmos_model(num_lines, num_pixels)float32...long_name :two-way atmospheric correction to sigma0 from modelsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :1valid_min :1.0valid_max :10.0comment :Atmospheric correction to sigma0 from weather model data as a linear power multiplier (not decibels). sig0_cor_atmos_model is already applied in computing sig0_karin_2.[700628 values with dtype=float32]sig0_cor_atmos_rad(num_lines, num_pixels)float32...long_name :two-way atmospheric correction to sigma0 from radiometer datasource :Advanced Microwave Radiometerunits :1valid_min :1.0valid_max :10.0comment :Atmospheric correction to sigma0 from radiometer data as a linear power multiplier (not decibels). sig0_cor_atmos_rad is already applied in computing sig0_karin.[700628 values with dtype=float32]doppler_centroid(num_lines, num_sides)float32...long_name :doppler centroid estimated by KaRInunits :1/svalid_min :-30000valid_max :30000comment :Doppler centroid (in hertz or cycles per second) estimated by KaRIn.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)phase_bias_ref_surface(num_lines, num_pixels)float64...long_name :height of reference surface used for phase bias calculationunits :mvalid_min :-15000000valid_max :150000000comment :Height (relative to the reference ellipsoid) of the reference surface used for phase bias calculation during L1B processing.[700628 values with dtype=float64]obp_ref_surface(num_lines, num_pixels)float64...long_name :height of reference surface used by on-board-processorunits :mvalid_min :-15000000valid_max :150000000comment :Height (relative to the reference ellipsoid) of the reference surface used by the KaRIn on-board processor.[700628 values with dtype=float64]rad_tmb_187(num_lines, num_sides)float32...long_name :radiometer main beam brightness temperature at 18.7 GHzstandard_name :toa_brightness_temperaturesource :Advanced Microwave Radiometerunits :Kvalid_min :13000valid_max :25000comment :Main beam brightness temperature measurement at 18.7 GHz. Value is unsmoothed (along-track averaging has not been performed).array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_tmb_238(num_lines, num_sides)float32...long_name :radiometer main beam brightness temperature at 23.8 GHzstandard_name :toa_brightness_temperaturesource :Advanced Microwave Radiometerunits :Kvalid_min :13000valid_max :25000comment :Main beam brightness temperature measurement at 23.8 GHz. Value is unsmoothed (along-track averaging has not been performed).array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_tmb_340(num_lines, num_sides)float32...long_name :radiometer main beam brightness temperature at 34.0 GHzstandard_name :toa_brightness_temperaturesource :Advanced Microwave Radiometerunits :Kvalid_min :15000valid_max :28000comment :Main beam brightness temperature measurement at 34.0 GHz. Value is unsmoothed (along-track averaging has not been performed).array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_water_vapor(num_lines, num_sides)float32...long_name :water vapor content from radiometerstandard_name :atmosphere_water_vapor_content source :Advanced Microwave Radiometerunits :kg/m^2valid_min :0valid_max :15000comment :Integrated water vapor content from radiometer measurements.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)rad_cloud_liquid_water(num_lines, num_sides)float32...long_name :liquid water content from radiometerstandard_name :atmosphere_cloud_liquid_water_content  source :Advanced Microwave Radiometerunits :kg/m^2valid_min :0valid_max :2000comment :Integrated cloud liquid water content from radiometer measurements.array([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       ...,\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]], dtype=float32)mean_sea_surface_cnescls(num_lines, num_pixels)float64...long_name :mean sea surface height (CNES/CLS)source :CNES_CLS_15institution :CNES/CLSunits :mvalid_min :-1500000valid_max :1500000comment :Mean sea surface height above the reference ellipsoid. The value is referenced to the mean tide system, i.e. includes the permanent tide (zero frequency).[700628 values with dtype=float64]mean_sea_surface_cnescls_uncert(num_lines, num_pixels)float32...long_name :mean sea surface height accuracy (CNES/CLS)source :CNES_CLS_15institution :CNES/CLSunits :mvalid_min :0valid_max :10000comment :Accuracy of the mean sea surface height (mean_sea_surface_cnescls).[700628 values with dtype=float32]mean_sea_surface_dtu(num_lines, num_pixels)float64...long_name :mean sea surface height (DTU)source :DTU18institution :DTUunits :mvalid_min :-1500000valid_max :1500000comment :Mean sea surface height above the reference ellipsoid. The value is referenced to the mean tide system, i.e. includes the permanent tide (zero frequency).[700628 values with dtype=float64]mean_sea_surface_dtu_uncert(num_lines, num_pixels)float32...long_name :mean sea surface height accuracy (DTU)source :DTU18institution :DTUunits :mvalid_min :0valid_max :10000comment :Accuracy of the mean sea surface height (mean_sea_surface_dtu)[700628 values with dtype=float32]geoid(num_lines, num_pixels)float64...long_name :geoid heightstandard_name :geoid_height_above_reference_ellipsoidsource :EGM2008 (Pavlis et al., 2012)units :mvalid_min :-1500000valid_max :1500000comment :Geoid height above the reference ellipsoid with a correction to refer the value to the mean tide system, i.e. includes the permanent tide (zero frequency).[700628 values with dtype=float64]mean_dynamic_topography(num_lines, num_pixels)float32...long_name :mean dynamic topographysource :CNES_CLS_18institution :CNES/CLSunits :mvalid_min :-30000valid_max :30000comment :Mean dynamic topography above the geoid.[700628 values with dtype=float32]mean_dynamic_topography_uncert(num_lines, num_pixels)float32...long_name :mean dynamic topography accuracysource :CNES_CLS_18institution :CNES/CLSunits :mvalid_min :0valid_max :10000comment :Accuracy of the mean dynamic topography.[700628 values with dtype=float32]depth_or_elevation(num_lines, num_pixels)float32...long_name :ocean depth or land elevationsource :Altimeter Corrected Elevations, version 2institution :European Space Agencyunits :mvalid_min :-12000valid_max :10000comment :Ocean depth or land elevation above reference ellipsoid. Ocean depth (bathymetry) is given as negative values, and land elevation positive values.[700628 values with dtype=float32]solid_earth_tide(num_lines, num_pixels)float32...long_name :solid Earth tide heightsource :Cartwright and Taylor (1971) and Cartwright and Edden (1973)units :mvalid_min :-10000valid_max :10000comment :Solid-Earth (body) tide height. The zero-frequency permanent tide component is not included.[700628 values with dtype=float32]ocean_tide_fes(num_lines, num_pixels)float64...long_name :geocentric ocean tide height (FES)source :FES2014b (Carrere et al., 2016)institution :LEGOS/CNESunits :mvalid_min :-300000valid_max :300000comment :Geocentric ocean tide height. Includes the sum total of the ocean tide, the corresponding load tide (load_tide_fes) and equilibrium long-period ocean tide height (ocean_tide_eq).[700628 values with dtype=float64]ocean_tide_got(num_lines, num_pixels)float64...long_name :geocentric ocean tide height (GOT)source :GOT4.10c (Ray, 2013)institution :GSFCunits :mvalid_min :-300000valid_max :300000comment :Geocentric ocean tide height. Includes the sum total of the ocean tide, the corresponding load tide (load_tide_got) and equilibrium long-period ocean tide height (ocean_tide_eq).[700628 values with dtype=float64]load_tide_fes(num_lines, num_pixels)float32...long_name :geocentric load tide height (FES)source :FES2014b (Carrere et al., 2016)institution :LEGOS/CNESunits :mvalid_min :-2000valid_max :2000comment :Geocentric load tide height. The effect of the ocean tide loading of the Earth's crust. This value has already been added to the corresponding ocean tide height value (ocean_tide_fes).[700628 values with dtype=float32]load_tide_got(num_lines, num_pixels)float32...long_name :geocentric load tide height (GOT)source :GOT4.10c (Ray, 2013)institution :GSFCunits :mvalid_min :-2000valid_max :2000comment :Geocentric load tide height. The effect of the ocean tide loading of the Earth's crust. This value has already been added to the corresponding ocean tide height value (ocean_tide_got).[700628 values with dtype=float32]ocean_tide_eq(num_lines, num_pixels)float32...long_name :equilibrium long-period ocean tide heightunits :mvalid_min :-2000valid_max :2000comment :Equilibrium long-period ocean tide height. This value has already been added to the corresponding ocean tide height values (ocean_tide_fes and ocean_tide_got).[700628 values with dtype=float32]ocean_tide_non_eq(num_lines, num_pixels)float32...long_name :non-equilibrium long-period ocean tide heightsource :FES2014b (Carrere et al., 2016)institution :LEGOS/CNESunits :mvalid_min :-2000valid_max :2000comment :Non-equilibrium long-period ocean tide height. This value is reported as a relative displacement with repsect to ocean_tide_eq. This value can be added to ocean_tide_eq, ocean_tide_fes, or ocean_tide_got, or subtracted from ssha_karin and ssha_karin_2, to account for the total long-period ocean tides from equilibrium and non-equilibrium contributions.[700628 values with dtype=float32]internal_tide_hret(num_lines, num_pixels)float32...long_name :coherent internal tide (HRET)source :Zaron (2019)units :mvalid_min :-2000valid_max :2000comment :Coherent internal ocean tide. This value is subtracted from the ssh_karin and ssh_karin_2 to compute ssha_karin and ssha_karin_2, respectively.[700628 values with dtype=float32]internal_tide_sol2(num_lines, num_pixels)float32...long_name :coherent internal tide (Model 2)source :TBDunits :mvalid_min :-2000valid_max :2000comment :Coherent internal tide.[700628 values with dtype=float32]pole_tide(num_lines, num_pixels)float32...long_name :geocentric pole tide heightsource :Wahr (1985) and Desai et al. (2015)units :mvalid_min :-2000valid_max :2000comment :Geocentric pole tide height.  The total of the contribution from the solid-Earth (body) pole tide height, the ocean pole tide height, and the load pole tide height (i.e., the effect of the ocean pole tide loading of the Earth's crust).[700628 values with dtype=float32]dac(num_lines, num_pixels)float32...long_name :dynamic atmospheric correctionsource :MOG2Dinstitution :LEGOS/CNES/CLSunits :mvalid_min :-12000valid_max :12000comment :Model estimate of the effect on sea surface topography due to high frequency air pressure and wind effects and the low-frequency height from inverted barometer effect (inv_bar_cor). This value is subtracted from the ssh_karin and ssh_karin_2 to compute ssha_karin and ssha_karin_2, respectively. Use only one of inv_bar_cor and dac.[700628 values with dtype=float32]inv_bar_cor(num_lines, num_pixels)float32...long_name :static inverse barometer effect on sea surface heightunits :mvalid_min :-2000valid_max :2000comment :Estimate of static effect of atmospheric pressure on sea surface height. Above average pressure lowers sea surface height. Computed by interpolating ECMWF pressure fields in space and time. The value is included in dac. To apply, add dac to ssha_karin and ssha_karin_2 and subtract inv_bar_cor.[700628 values with dtype=float32]model_dry_tropo_cor(num_lines, num_pixels)float32...long_name :dry troposphere vertical correctionsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mvalid_min :-30000valid_max :-15000comment :Equivalent vertical correction due to dry troposphere delay. The reported sea surface height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height.[700628 values with dtype=float32]model_wet_tropo_cor(num_lines, num_pixels)float32...long_name :wet troposphere vertical correction from weather model datasource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mvalid_min :-10000valid_max :0comment :Equivalent vertical correction due to wet troposphere delay from weather model data. The reported pixel height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height (ssh_karin_2) results in the uncorrected sea surface height.[700628 values with dtype=float32]rad_wet_tropo_cor(num_lines, num_pixels)float32...long_name :wet troposphere vertical correction from radiometer datasource :Advanced Microwave Radiometerunits :mvalid_min :-10000valid_max :0comment :Equivalent vertical correction due to wet troposphere delay from radiometer measurements. The reported pixel height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height (ssh_karin) results in the uncorrected sea surface height.[700628 values with dtype=float32]iono_cor_gim_ka(num_lines, num_pixels)float32...long_name :ionosphere vertical correctionsource :Global Ionosphere Mapsinstitution :JPLunits :mvalid_min :-5000valid_max :0comment :Equivalent vertical correction due to ionosphere delay. The reported sea surface height, latitude and longitude are computed after adding negative media corrections to uncorrected range along slant-range paths, accounting for the differential delay between the two KaRIn antennas. The equivalent vertical correction is computed by applying obliquity factors to the slant-path correction. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height.[700628 values with dtype=float32]height_cor_xover(num_lines, num_pixels)float64...long_name :height correction from KaRIn crossoversunits :mvalid_min :-100000valid_max :100000comment :Height correction from KaRIn crossover calibration. To apply this correction the value of height_cor_xover should be added to the value of ssh_karin, ssh_karin_2, ssha_karin, and ssha_karin_2.[700628 values with dtype=float64]correction_flag(num_lines, num_pixels)float32...long_name :quality flag for correctionsstandard_name :status_flagflag_meanings :good badflag_values :[0 1]valid_min :0valid_max :1comment :Quality flag for corrections.[700628 values with dtype=float32]rain_rate(num_lines, num_pixels)float32...long_name :rain rate from weather modelsource :European Centre for Medium-Range Weather Forecastsinstitution :ECMWFunits :mm/hrvalid_min :0valid_max :200comment :Rain rate from weather model.[700628 values with dtype=float32]ice_conc(num_lines, num_pixels)float32...long_name :concentration of sea icestandard_name :sea_ice_area_fractionsource :EUMETSAT Ocean and Sea Ice Satellite Applications Facilityinstitution :EUMETSATunits :%valid_min :0valid_max :10000comment :Concentration of sea ice from model.[700628 values with dtype=float32]sea_state_bias_cor(num_lines, num_pixels)float32...long_name :sea state bias correction to heightsource :TBDunits :mvalid_min :-6000valid_max :0comment :Sea state bias correction to ssh_karin. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height. The wind_speed_karin value is used to compute this quantity.[700628 values with dtype=float32]sea_state_bias_cor_2(num_lines, num_pixels)float32...long_name :sea state bias correction to heightsource :TBDunits :mvalid_min :-6000valid_max :0comment :Sea state bias correction to ssh_karin_2. Adding the reported correction to the reported sea surface height results in the uncorrected sea surface height. The wind_speed_karin_2 value is used to compute this quantity.[700628 values with dtype=float32]swh_sea_state_bias(num_lines, num_pixels)float32...long_name :SWH used in sea state bias correctionunits :mvalid_min :0valid_max :25000comment :Significant wave height used in sea state bias correction.[700628 values with dtype=float32]simulated_true_ssh_karin(num_lines, num_pixels)float64...long_name :sea surface heightstandard_name :sea surface height above reference ellipsoidunits :mvalid_min :-15000000valid_max :150000000comment :Height of the sea surface free of measurement errors.[700628 values with dtype=float64]simulated_error_baseline_dilation(num_lines, num_pixels)float64...long_name :Error due to baseline mast dilationunits :m[700628 values with dtype=float64]simulated_error_timing(num_lines, num_pixels)float64...long_name :Timing errorunits :m[700628 values with dtype=float64]simulated_error_roll(num_lines, num_pixels)float64...long_name :Error due to rollunits :m[700628 values with dtype=float64]simulated_error_phase(num_lines, num_pixels)float64...long_name :Error due to phaseunits :m[700628 values with dtype=float64]simulated_error_karin(num_lines, num_pixels)float64...long_name :KaRIn errorunits :m[700628 values with dtype=float64]simulated_error_orbital(num_lines, num_pixels)float64...long_name :Error due to orbital perturbationsunits :m[700628 values with dtype=float64]simulated_error_troposphere(num_lines, num_pixels)float64...long_name :Error due to wet troposphere path delayunits :m[700628 values with dtype=float64]Attributes: (32)Conventions :CF-1.7title :Level 2 Low Rate Sea Surface Height Data Product - Expert SSH with Wind and Waveinstitution :CNES/JPLsource :Simulate producthistory :2021-09-23 07:47:11Z : Creationplatform :SWOTproduct_version :1.2.1.dev10references :Gaultier, L., C. Ubelmann, and L.-L. Fu, 2016: The Challenge of Using Future SWOT Data for Oceanic Field Reconstruction. J. Atmos. Oceanic Technol., 33, 119-126, doi:10.1175/jtech-d-15-0160.1. http://dx.doi.org/10.1175/JTECH-D-15-0160.1.reference_document :D-56407_SWOT_Product_Description_L2_LR_SSHcontact :CNES aviso@altimetry.fr, JPL podaac@podaac.jpl.nasa.govcycle_number :4pass_number :17equator_time :2011-11-16T13:34:56.899302Zequator_longitude :228.16567754818402time_coverage_start :2011-11-16T13:09:22.641983Ztime_coverage_end :2011-11-16T14:00:28.235110Zgeospatial_lon_min :144.76234679489446geospatial_lon_max :311.9031061982219geospatial_lat_min :-78.2920195134443geospatial_lat_max :78.29270807524945left_first_longitude :144.76234679489446left_first_latitude :-77.03297084205963left_last_longitude :311.9031061982219left_last_latitude :78.29270807524945right_first_longitude :144.7714992348211right_first_latitude :-78.2920195134443right_last_longitude :311.89957599684425right_last_latitude :77.03365811434979wavelength :0.008385803020979orbit_solution :POEellipsoid_semi_major_axis :6378137.0ellipsoid_flattening :0.0033528106647474805\n\n\nLet us look at one error field “simulated_error_troposphere”.\n\ndata['simulated_error_troposphere']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'simulated_error_troposphere' (num_lines: 9868, num_pixels: 71)>\n[700628 values with dtype=float64]\nCoordinates:\n    latitude         (num_lines, num_pixels) float64 ...\n    longitude        (num_lines, num_pixels) float64 ...\n    latitude_nadir   (num_lines) float64 -77.66 -77.66 -77.66 ... 77.66 77.66\n    longitude_nadir  (num_lines) float64 144.8 144.9 144.9 ... 311.7 311.8 311.9\nDimensions without coordinates: num_lines, num_pixels\nAttributes:\n    long_name:  Error due to wet troposphere path delay\n    units:      mxarray.DataArray'simulated_error_troposphere'num_lines: 9868num_pixels: 71...[700628 values with dtype=float64]Coordinates: (4)latitude(num_lines, num_pixels)float64...long_name :latitude (positive N, negative S)standard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Latitude of measurement [-80,80]. Positive latitude is North latitude, negative latitude is South latitude.[700628 values with dtype=float64]longitude(num_lines, num_pixels)float64...long_name :longitude (degrees East)standard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude of measurement. East longitude relative to Greenwich meridian.[700628 values with dtype=float64]latitude_nadir(num_lines)float64-77.66 -77.66 ... 77.66 77.66long_name :latitude of satellite nadir pointstandard_name :latitudeunits :degrees_northvalid_min :-80000000valid_max :80000000comment :Geodetic latitude [-80,80] (degrees north of equator) of the satellite nadir point.array([-77.662495, -77.662454, -77.662388, ...,  77.663153,  77.663181,\n        77.663183])longitude_nadir(num_lines)float64144.8 144.9 144.9 ... 311.8 311.9long_name :longitude of satellite nadir pointstandard_name :longitudeunits :degrees_eastvalid_min :0valid_max :359999999comment :Longitude (degrees east of Grenwich meridian) of the satellite nadir point.array([144.766698, 144.850876, 144.935054, ..., 311.732881, 311.817064,\n       311.901247])Attributes: (2)long_name :Error due to wet troposphere path delayunits :m\n\n\nThe following line returns a list of filenames with the full S3 link within the collection SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1, the simulated KaRIn L2 swaths on the science orbit based on ECCO_LLC4320.\n\nfns=s3sys.glob(\"s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1/*nc\")\n\nPlot eight simulated L2 fields including the model truth and simulated errors.\n\nThere may be a bug in creating the timing error by the new simulator. According to SWOT_D-79084, the timing drift error, to the first order, creates a constant height bias across the swath. It accounts for 10% of the overall error budget. The first version of the SWOTsimulator correctly implemented it but not in the second simulator. Currently I am communicting with the CNES team.\n\n\ndata=open_swot_L2SSH(fns[100])\nfig,ax=plt.subplots(2,4,figsize=(20,10),sharex=True)\n\naxx=ax.flatten()\n\nkeys=[]\nfor key in data.keys():\n    if 'simulated' in key:\n        keys.append(key)\nprint(keys)\nfor i, key in enumerate(keys):\n    data[key][1000:1100,:].plot(ax=axx[i],)\n    axx[i].set_title(key)\nplt.tight_layout()\n\n['simulated_true_ssh_karin', 'simulated_error_baseline_dilation', 'simulated_error_roll', 'simulated_error_phase', 'simulated_error_timing', 'simulated_error_karin', 'simulated_error_orbital', 'simulated_error_troposphere']\n\n\n\n\n\n\n#Plot an example of one pass\n\nimport os\nos.getcwd()\nimport pylab as plt\nimport numpy as np\n\ndd=s3sys.open(fns[1000])\nds=xr.open_dataset(dd)\n\n#print(ds.keys())\n\n#ds=nc.Dataset(\"SWOT_L2_LR_SSH_Expert_018_290_20121112T003212_20121112T012339_DG10_01.nc\")\n#print(ds.variables.keys())\n\nsla=ds['simulated_true_ssh_karin'].data.flatten()\nlat=ds['latitude'].data.flatten()\nlon=ds['longitude'].data.flatten()\n\n#mask=(lat>-40)&(lat<-20)&(lon>90)&(lon<96)\n#sla=np.ma.masked_array(sla,mask=~mask)\n#sla=np.ma.masked_invalid(sla)\n#print(sla.mean())\n#sla=sla-sla.mean()\n\nplt.figure(figsize=(10,6))\nplt.scatter(lon,lat,c=sla,cmap=plt.cm.jet,s=1)\nplt.colorbar(shrink=0.5)\n#plt.xlim(90,96)\n#plt.ylim(-40,-20)\n\n<matplotlib.colorbar.Colorbar at 0x7fb08f856af0>\n\n\n\n\n\n\nSave all passes into a signal file (experimental)\nI used the following to save the lat/lon/time information from all passes and into a single (big) file. You can use it as a template to save other variables.\n%%timeit\n#KaRIn LLC4320 Science orbit s3path=“s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_*nc”\n#s3path=“podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1/*nc”\nfns= s3sys.glob(s3path) print(len(fns),fns[0])\ndd=s3sys.open(fns[0]) ds=xr.open_dataset(dd) msk=~np.isnan(ds[‘simulated_error_baseline_dilation’].data[0,:]) lon=ds[‘longitude’][:,msk].values.astype(‘>f4’) lat=ds[‘latitude’][:,msk].values.astype(‘>f4’) time=ds[‘time’].values\ndel dd, ds\nlons=xr.DataArray(lon,name=‘Longitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time}) lats=xr.DataArray(lat,name=‘Latitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time})\nfor fn in fns[1:]: #print(fn) dd=s3sys.open(fn) ds=xr.open_dataset(dd) lon=ds[‘longitude’][:,msk].values.astype(‘>f4’) lat=ds[‘latitude’][:,msk].values.astype(‘>f4’) time=ds[‘time’].values lon=xr.DataArray(lon,name=‘Longitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time}) lat=xr.DataArray(lat,name=‘Latitude’,dims=(‘time’,‘num_pixels’),coords={‘time’:time}) lons=xr.concat([lons,lon],dim=‘time’) lats=xr.concat([lats,lat],dim=‘time’) del lon,lat, dd, ds, time\nxr.Dataset({‘Longitude’:lons,‘Latitude’:lats}).to_netcdf(‘SWOT_KaRIn_CALVAL_LatLon.nc’)\n\nPlease engage with PODAAC by sharing your experience, sending feedback, and asking questions, either through podaac@podaac.jpl.nasa.gov or any of the PODAAC team members who you feel comfortable to communicate. Engagement leads to better services. Go SWOT!"
  },
  {
    "objectID": "tutorials/02_Subset_SWOT_sim.html",
    "href": "tutorials/02_Subset_SWOT_sim.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "Use a Harmony request to obtain a temporal and spatial subset of L2 altimetric data in the cloud.\nConstruct and submit the Harmony request by using the harmony-py library.\nOpen the data in xarray and make a simple plot to visually confirm the download and subset.\n\n\n\n\n\nThe advantage of reducing data size for access/download, especially for long global time series.\nHarmony allows us to access data from different NASA DAACs in a consistent way (not DAAC-specific).\nA number of different services like spatial subset, variable subset, etc., can all be called from Harmony\n\n\n\n\n\nNadir GLORYS CalVal: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_CALVAL_V1\nNadir GLORYS Science: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1\nNadir ECCO CalVal: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_CALVAL_V1\nNadir ECCO Science: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_SCIENCE_V1\nKaRIn GLORYS CalVal: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1\nKaRIn GLORYS Science: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1\nKaRIn ECCO CalVal: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1\nKaRIn ECCO Science: https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1\n\n\n\nNote: Install harmony-py to your Python environment before you can import it for the first time. See https://github.com/nasa/harmony-py (For the SWOT Ocean Cloud Wokrshop March 2022, the needed libraries have been pre-loaded and installed in the cloud compute environment.)\n\nimport xarray as xr\nimport numpy as np\nfrom IPython.display import display, JSON\nfrom datetime import datetime, timedelta, time\nimport os\n\nfrom harmony import BBox, Client, Collection, Request, Environment, LinkType \n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n%matplotlib inline\n\n\n\n\n\n# Start the Harmony Client.\n\nharmony_client = Client(env=Environment.PROD)\n\n# \"PROD\" stands for production. This is the environment for users.\n\n\n\n\n\n\nDefine the collection of interest by calling Collection(id = YourCollection), where YourCollection is a collection short name or concept-id. There are a number of ways to get the collection shortname; using Earthdata Search is one way - see pre-workshop tutorial.\nSet time bounds.\nSet spatial bounding box.\nThere are also other options such as variables, granules, and concatenation.\n\n\ncollection = Collection(id='SWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1')\n\nstart_day = datetime(2015,4,15,0,0,0)\nend_day = datetime(2015,4,20,0,0,0)\n\nrequest = Request(\n    collection=collection,\n    temporal={\n        'start': start_day,\n        'stop': end_day\n    },\n    spatial=BBox(-140, 20, -100, 50), # [20-50N], [140W-100W] CA Current crossover point (35N,125W)\n    # variables=[],\n    # granule_id=granuleIDs,\n    # concatenate = True,\n)\n\nrequest.is_valid()\n\n\nprint(harmony_client.request_as_curl(request))\njob_id = harmony_client.submit(request)\nprint(f'Job ID: {job_id}') # This job id is shareable:show how to do this\n\n\n\nA Harmony request is limited to 200 granules. The limit is there to prevent users from accidentally make huge requests.\n\nharmony_client.status(job_id) \n\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\n\n\n\nFilenames that end with “subsetted.nc4” have been subsetted.\nThe other filenames (that are un-altered) indicate that these were rounded up as relevant files during Harmony’s search, but do not contain data in the actual region of interest, so the files downloaded here are empty.\n\n# create a new folder to put the subsetted data in\nos.makedirs(\"swot_ocean_basic_subset\",exist_ok = True)\n\n\nfutures = harmony_client.download_all(job_id, directory='./swot_ocean_basic_subset', overwrite=True)\nfile_names = [f.result() for f in futures]\nsorted(file_names)\n\n\nfrom os import listdir\nfrom os.path import isfile, join\ndata_files = [ f for f in file_names if \"subsetted\" in f]\ndata_files\n\n\n\n\nNote: xarray is a little clunky with variables in groups.\n\nds = xr.open_mfdataset(sorted(data_files),combine='nested',concat_dim='time',group='data_01')\nds\n\n\nfig = plt.figure(figsize=[11,7]) \nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\nax.set_extent([-150, -90, 10, 60])\nplt.scatter(ds.longitude, ds.latitude, lw=1, c=ds.depth_or_elevation)\nplt.colorbar(label='Depth or elevation (m)')\nplt.clim(-4000,4000)\nplt.show()\n# ds.plot.scatter( y=\"latitude\",\n#                  x=\"longitude\", \n#                  hue=\"depth_or_elevation\",\n#                  s=1,\n#                  vmin=-4000,\n#                  vmax=4000,\n#                  levels=9, \n#                  cmap=\"jet\",\n#                  aspect=2.5,\n#                  size=9, )"
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "Jinbo Wang\nSWOT Scientist/PODAAC Project Scientist\n3/15/2022"
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#simulated-l2-ssh",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#simulated-l2-ssh",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Simulated L2 SSH",
    "text": "Simulated L2 SSH\nThe SWOTsimulator was used on two global ocean simulations (LLC4320 and GLORYS) following the error specification described in Level 2 KaRIn Low Rate Sea Surface Height Product PDF file (D-56407). The simulator was based on Gaultier et al. first version but was almost completely rewritten, while keeping the error budget largely unchanged. (The error representation is claimed to be better but needs more documentation.) The CNES team is the creator of these products. The 8 datasets are listed as follows:\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_CALVAL_V1          17686 files\nSWOT_SIMULATED_L2_NADIR_SSH_GLORYS_SCIENCE_V1         17564 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_CALVAL_V1    10288 files\nSWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_CALVAL_V1    10287 files\nSWOT_SIMULATED_L2_NADIR_SSH_ECCO_LLC4320_SCIENCE_V1   10218 files\nThe links to these datasets on PODAAC website or Earthdata search page.\n\nEight simulated L2 SSH datasets on PO.DAAC website.\nFour GLORYS datasets from Earthdata search\nFour ECCO_LLC4320 datasets from Earthdata search\n\nThe ECCO_LLC4320-based products cover one-year duration with >10k files (granules in Earthdata language) in each collection. The GLORYS-based products cover about 20 months with >17k files in each collection."
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#relevant-content-in-the-simulated-l2-ssh-files",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#relevant-content-in-the-simulated-l2-ssh-files",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Relevant content in the simulated L2 SSH files",
    "text": "Relevant content in the simulated L2 SSH files\nThere are too much information in these datasets for this short cloud hackathon. For example, there are 92 variables in the KaRIn products. The hackathon participants are strongly encouraged to read the the product description to understand the meaning of different variables. For this exercise, the variables with “simulated” keyword are most relevant: 1. ‘simulated_true_ssh_karin’ 1. ‘simulated_error_baseline_dilation’ 1. ‘simulated_error_roll’ 1. ‘simulated_error_phase’ 1. ‘simulated_error_timing’ 1. ‘simulated_error_karin’ 1. ‘simulated_error_orbital’ 1. ‘simulated_error_troposphere’"
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#subsetting-using-dask-delay",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#subsetting-using-dask-delay",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Subsetting using Dask delay",
    "text": "Subsetting using Dask delay\nSteps to use Dask within jupyter-notebook 1. Go to the Dask extension (third button on the left-hand side vertical bar). 1. Start a Dask cluster by clicking “+” NEW. You will get the following: \n\nUse “SCALE” to maximumize the number of workers according to your AWS instance.\n\nCopy tcp://127.0.0.1:????? link from the LocalCluster and replace the following scheduler variable.\n\n\nscheduler='tcp://127.0.0.1:34721'\nout_folder='/home/jovyan/podaac_hackathon_swot_20220316/subsets/' #change this to your folder\n\n\n#The 8 datasets have short names starting from \"SWOT\". \n#You may get more than these 8 collections in the future after new SWOT data ingested. \n#The following wildcard will give you the 8 collections for now.\n#%%timeit\n\ns3sys=init_S3FileSystem()\n\ns3path=\"s3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_ECCO_LLC4320_SCIENCE_V1/*nc\"  \n#Search all collections that fit to the wildcard.\nfns= s3sys.glob(s3path)\nprint(len(fns))\n\nswot_karin_l2_subset_dask(scheduler,fns[:1000], \n                          'simulated_true_ssh_karin', \n                          [-126,-120,30,50], \n                           out_folder=out_folder )"
  },
  {
    "objectID": "tutorials/swot_cloud_hackathon_data_intro_dask.html#print-the-content-of-an-example-file",
    "href": "tutorials/swot_cloud_hackathon_data_intro_dask.html#print-the-content-of-an-example-file",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "Print the content of an example file",
    "text": "Print the content of an example file\nLoad and plot one subset.\n\nfrom glob import glob\nfns_local=sorted(glob('subsets/*nc'))\n\ndata=xr.open_dataset(fns_local[10])\ndata\n\nLet us look at one error field “simulated_error_troposphere”.\n\nplt.scatter(data['longitude'],data['latitude'],s=1,c=data['simulated_true_ssh_karin'])\n\n\nPlease engage with PODAAC by sharing your experience, sending feedback, and asking questions, either through podaac@podaac.jpl.nasa.gov or any of the PODAAC team members who you feel comfortable to communicate. Engagement leads to better services. Go SWOT!"
  },
  {
    "objectID": "tutorials/On-prem_Cloud_example.html",
    "href": "tutorials/On-prem_Cloud_example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "This tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics.\n\n\n\n\n\nAWS instance running in us-west 2\nEarthdata Login\n.netrc file\n\n\n\n\n\n\nSearch for data programmatically using the Common Metadata Repository (CMR), determining granule (file) coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation.\n\n\n\n\n\n\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access via download\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample\n\n\n\n\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below.\nThe same search and access steps for both datasets can be performed via Earthdata Search using the same spatial and temporal filtering options. See the Earthdata Search tutorial for more information on how to use Earthdata Search to discover and access data from the Earthdata Cloud.\n\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nSee the Data Discovery with CMR tutorial for more details on how to navigate the NASA Common Metadata Repository (CMR) Application Programming Interface, or API. For some background, the CMR catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). The CMR API allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\nFor this tutorial, we have already identified the unique identifier, or concept_id for each dataset:\n\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2153574585-NSIDC_CPRD'\n\nThis Earthdata Search Project also provides the same data access links that we will identify in the following steps for each dataset (note that you will need an Earthdata Login account to access this project).\n\n\n\nPerform a granule search over our time and area of interest. How many granules are returned?\n\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_005_01.h5 242.02353763580322 https://data.nsidc.earthdatacloud.nasa.gov/nsidc-cumulus-prod-protected/ATLAS/ATL07/005/2019/06/22/ATL07-01_20190622055317_12980301_005_01.h5\nATL07-01_20190622200154_13070301_005_01.h5 237.13062953948975 https://data.nsidc.earthdatacloud.nasa.gov/nsidc-cumulus-prod-protected/ATLAS/ATL07/005/2019/06/22/ATL07-01_20190622200154_13070301_005_01.h5\n\n\n\n\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\nr = requests.get(icesat_url)\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nx-amz-id-2: 1yo1aVdE3HHWp/fjVPe9AUN5Rs8iHHJPK6wJVZKNmh1hw1X6IgirTDkc5YWN8ANYyTk2rMi62V0=\nx-amz-request-id: ARWMK1FBA300Z6WX\nDate: Wed, 19 Oct 2022 23:45:26 GMT\nLast-Modified: Wed, 22 Dec 2021 02:11:20 GMT\nETag: \"89e9409795707b0155ced7488c3abc41-1\"\nx-amz-server-side-encryption: AES256\nAccept-Ranges: bytes\nContent-Type: binary/octet-stream\nServer: AmazonS3\nContent-Length: 253780073\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\nos.makedirs(\"downloads\", exist_ok=True)\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\noutfile = Path('downloads', icesat_id)\n\n\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data < attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (segment: 235812)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4229 -0.425xarray.DatasetDimensions:segment: 235812Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38429082, 82.38429082, 82.38429082, ..., 72.60985656,\n       72.6097838 , 72.60971985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10889805, -55.10889805, -55.10889805, ..., 145.05396636,\n       145.05393262, 145.05390295])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.6421954 , 46419293.6421954 , 46419293.6421954 , ...,\n       46419681.87630081, 46419681.87745472, 46419681.87846863])height_segment_height(segment)float32nan nan nan ... -0.4229 -0.425_FillValue :[3.4028235e+38]contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.45860034,\n       -0.4228644 , -0.42503005], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;\n\n\n\n\n\n\n\n\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n19\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622131501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.143062591552734 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622131501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622145501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.070895195007324 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622145501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622195000-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622195000-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622213000-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622213000-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n\n\n\n\n\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\n\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\n\n\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :reference time of sst filestandard_name :timeunits :seconds since 1981-01-01 00:00:00comment :time of first sensor observationcoverage_content_type :coordinatearray([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_north_FillValue :-999.0valid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_east_FillValue :-999.0valid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvin_FillValue :-32767valid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelsscale_factor :0.005add_offset :273.15source :NASA and University of Miamicoordinates :lon latcoverage_content_type :physicalMeasurement[2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :time difference from reference timeunits :seconds_FillValue :-32768valid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coordinates :lon latcoverage_content_type :referenceInformation[2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :quality level of SST pixel_FillValue :-128valid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoordinates :lon latflag_values :[0 1 2 3 4 5]flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :SSES bias error based on proximity confidence flagsunits :kelvin_FillValue :-128valid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuescale_factor :0.15748031add_offset :0.0coordinates :lon latcoverage_content_type :auxiliaryInformation[2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvin_FillValue :-128valid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuescale_factor :0.07874016add_offset :10.0coordinates :lon latcoverage_content_type :auxiliaryInformation[2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablescoordinates :lon latflag_meanings :microwave land ice lake riverflag_masks :[ 1  2  4  8 16]coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3_FillValue :-32767.0valid_min :0.001valid_max :100.0comment :non L2P core fieldcoordinates :lon latcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1_FillValue :-32767valid_min :50valid_max :30000comment :non L2P core fieldscale_factor :0.0002add_offset :0.0coordinates :lon latcoverage_content_type :auxiliaryInformation[2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :10m wind speedstandard_name :wind_speedunits :m s-1_FillValue :-128valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacescale_factor :0.2add_offset :25.0source :TBD.  Placeholder.  Currently emptycoordinates :lon latgrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :deviation from SST reference climatologyunits :kelvin_FillValue :-128valid_min :-127valid_max :127comment :TBDscale_factor :0.1add_offset :0.0source :TBD. Placeholder.  Currently emptycoordinates :lon latcoverage_content_type :auxiliaryInformation[2748620 values with dtype=int8]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value, bytes):\n            da.attrs[attr] = value.decode('utf-8')\n        else:\n            da.attrs[attr] = value\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\nmodis_ds = xr.decode_cf(modis_ds)\n\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :[0 1 2 3 4 5]flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :[ 1  2  4  8 16]coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\n\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n<matplotlib.colorbar.Colorbar at 0x7f5aa84bc580>\n\n\n\n\n\n\n\n\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\n\n\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (segment: 235812)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4229 -0.425\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235812Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38429082, 82.38429082, 82.38429082, ..., 72.60985656,\n       72.6097838 , 72.60971985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10889805, -55.10889805, -55.10889805, ..., 145.05396636,\n       145.05393262, 145.05390295])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.6421954 , 46419293.6421954 , 46419293.6421954 , ...,\n       46419681.87630081, 46419681.87745472, 46419681.87846863])height_segment_height(segment)float32nan nan nan ... -0.4229 -0.425_FillValue :[3.4028235e+38]contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.45860034,\n       -0.4228644 , -0.42503005], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\n\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()\n\n<matplotlib.legend.Legend at 0x7f5ab2a20400>"
  },
  {
    "objectID": "tutorials/00_Setup.html",
    "href": "tutorials/00_Setup.html",
    "title": "00. Setup for tutorials",
    "section": "",
    "text": "This tutorial will help you set up your JupyterHub (or Hub) with tutorials and other materials from our Cloud Workshop folder.."
  },
  {
    "objectID": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "title": "00. Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to Jupyter Hub and Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\nSee how my Desktop is setup\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-cloud-environment",
    "href": "tutorials/00_Setup.html#discussion-cloud-environment",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "href": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "title": "00. Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\nWorkshop Book (my teaching notes, your reference material)\nZoom Chat"
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/00_Setup.html#step-2.-jupyterhub-orientation",
    "title": "00. Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/00_Setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/00_Setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "00. Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2021-Cloud-Workshop-AGU is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/00_Setup.html#jupyter-notebooks",
    "href": "tutorials/00_Setup.html#jupyter-notebooks",
    "title": "00. Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "href": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "title": "00. Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?) When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and click “Log Out”!\n\n\n\nhub-control-panel-button (credit: UW Hackweek)\n\n\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "In this notebook will show direct access of PO.DAAC archived products in the Earthdata Cloud in AWS Simple Storage Service (S3). In this demo, we will showcase the usage of SWOT Simulated Level-2 KaRIn SSH from GLORYS for Science Version 1. More information on the datasets can be found at https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF files into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\nIn the future, if you want to use this notebook as a reference, please note that we are not doing collection discovery here - we assume the collection of interest has been determined.\n\n\nThis can run in the Small openscapes instance, that is, it only needs 8GB of memory and ~2 CPU.\nIf you want to run this in your own AWS account, you can use a t2.large instance, which also has 2 CPU and 8GB memory. It’s improtant to note that all instances using direct S3 access to PO.DAAC or Earthdata data are required to run in us-west-2, or the Oregon region.\nThis instance will cost approximately $0.0832 per hour. The entire demo can run in considerably less time.\n\n\n\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\nboto3\ns3fs\nxarray\nmatplotlib\ncartopy\n\n\n\n\n\nimport needed libraries\nauthenticate for NASA Earthdata archive (Earthdata Login) (here this takes place as part of obtaining the AWS credentials step)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data by downloading directly into your cloud workspace from S3 within US-west 2 and operating on those files.\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\nNote: no files are being donwloaded off the cloud, rather, we are working with the data in the AWS cloud.\n\nimport boto3\nimport json\nimport xarray as xr\nimport s3fs\nimport os\nimport requests\nimport cartopy.crs as ccrs\nfrom matplotlib import pyplot as plt\nfrom os import path\n%matplotlib inline\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects (i.e. data) from applicable Earthdata Cloud buckets (storage space). For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs.\nThe below methods (get_temp_dreds) requires the user to have a ‘netrc’ file in the users home directory.\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\nWe will now get a credential for the ‘PO.DAAC’ provider and set up our environment to use those values.\nNOTE if you see an error like ‘HTTP Basic: Access denied.’ It means the username/password you’ve entered is incorrect.\nNOTE2 If you get what looks like a long HTML page in your error message (e.g. \n\n…), the right netrc ‘machine’ might be missing.\n\ncreds = get_temp_creds('podaac')\n\nos.environ[\"AWS_ACCESS_KEY_ID\"] = creds[\"accessKeyId\"]\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = creds[\"secretAccessKey\"]\nos.environ[\"AWS_SESSION_TOKEN\"] = creds[\"sessionToken\"]\n\ns3 = s3fs.S3FileSystem(anon=False) \n\n\n\n\nWe need to determine the path for our products of interest. We can do this through several mechanisms. Those are described in the Finding_collection_concept_ids.ipynb notebook, or the Pre-Workshop material, https://podaac.github.io/2022-SWOT-Ocean-Cloud-Workshop/prerequisites/01_Earthdata_Search.html.\nAfter using the Finding_collection_concept_ids.ipynb guide to find our S3 location, we end up with:\n{\n    ...\n    \"DirectDistributionInformation\": {\n        \"Region\": \"us-west-2\",\n        \"S3BucketAndObjectPrefixNames\": [\n            \"podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\",\n            \"podaac-ops-cumulus-public/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\"\n        ],\n        \"S3CredentialsAPIEndpoint\": \"https://archive.podaac.earthdata.nasa.gov/s3credentials\",\n        \"S3CredentialsAPIDocumentationURL\": \"https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME\"\n    },\n    ...\n}\n\n\n\nIt’s time to find our data! Below we are using a glob to find file names matching a pattern. Here, we want any files matching the pattern used below; here this equates, in science, terms, to Cycle 001 and the first 10 passes. This information can be gleaned from product description documents. Another way of finding specific data files would be to search on cycle/pass from CMR or Earthdata Search GUI and use the S3 links provided in the resulting metadata or access links, respectively, directly instead of doing a glob (essentially an ‘ls’).\nThe files we are looking at are about 11-13 MB each. So the 10 we’re looking to access are about ~100 MB total.\n\ns3path = 's3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_00*.nc'\nremote_files = s3.glob(s3path)\n\n\nremote_files\n\n['podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_001_20140412T120000_20140412T125126_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_002_20140412T125126_20140412T134253_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_003_20140412T134253_20140412T143420_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_004_20140412T143420_20140412T152546_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_005_20140412T152547_20140412T161713_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_006_20140412T161714_20140412T170840_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_007_20140412T170840_20140412T180007_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_008_20140412T180008_20140412T185134_DG10_01.nc',\n 'podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_009_20140412T185134_20140412T194301_DG10_01.nc']"
  },
  {
    "objectID": "tutorials/01_Direct_Access_SWOT_sim.html#a-final-word",
    "href": "tutorials/01_Direct_Access_SWOT_sim.html#a-final-word",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "A final word…",
    "text": "A final word…\nAccessing data completely from S3 and in memory are affected by various things.\n\nThe format of the data - archive formats like NetCDF, GEOTIFF, HDF vs cloud optimized data structures (Zarr, kerchunk, COG). Cloud formats are made for accessing only the pieces of data of interest needed at the time of the request (e.g. a subset, timestep, etc).\nThe internal structure of the data. Tools like xarray make a lot of assumptions about how to open and read a file. Sometimes the internals don’t fit the xarray ‘mould’ and we need to continue to work with data providers and software providers to make these two sides work together. Level 2 data (non-gridded), specifically, suffers from some of the assumptions made."
  },
  {
    "objectID": "tutorials/03_Timeseries_SWOT_sim.html",
    "href": "tutorials/03_Timeseries_SWOT_sim.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "In additional to libraries from the first tutorial, also import libraries needed to submit CMR requests.\n\nimport xarray as xr\nimport numpy as np\nfrom IPython.display import display, JSON\nfrom datetime import datetime, timedelta, time\nimport os\n\n# highlight the harmony-py library\nfrom harmony import BBox, Client, Collection, Request, Environment, LinkType \n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\n%matplotlib inline\n\n# Additional libraries compared to the first tutorial.\nfrom urllib import request\nimport json\nimport requests\nimport sys\nimport shutil\nfrom urllib.parse import urlencode\n\n\n\n\n\nharmony_client = Client(env=Environment.PROD)\ncmr_root = 'cmr.earthdata.nasa.gov'\n\n\n\n\n\n\nCMR Search: Number of item returned is limited to 10,000 (or 1 million if targeting collections) https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#paging-details\n\nprovider = 'POCLOUD'\nshort_name = 'SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_CALVAL_V1'\n\ntarget_cycle = list(range(1,101))\ntarget_pass = [2, 17]\npage_size = 2000\n\ngranuleIDs = []\nparams_without_cycle = [\n    ('page_size', page_size),\n    ('sort_key', \"-start_date\"),\n    ('provider', provider),\n    ('ShortName', short_name),\n    #('collection_concept_id', ccid), \n    #('token', token),\n    #('cycle', targetcycle), # can only specify one cycle at a time when specifying passes \n    ('passes[0][pass]', target_pass[0]), \n    ('passes[1][pass]', target_pass[1]),\n]\n\nfor v in target_cycle:\n    params = [(\"cycle[]\", v)]\n    params.extend(params_without_cycle)\n    # print(params)\n    query = urlencode(params)\n    cmr_url = f\"https://{cmr_root}/search/granules.umm_json?{query}\"\n    # print(cmr_url)\n    response = requests.get(cmr_url)\n    response_body = response.json()\n    \n    for itm in response_body['items']:\n        granuleIDs.append(itm['meta']['concept-id'])\n\nlen(granuleIDs) # Note the 200-granule limit\n\n\n\n\nOn the back end, the subsetting part of Harmony is powered by L2SS-py\n\n# collection = Collection(id=ccid)\ncollection = Collection(id=short_name)\n\n# start_day = datetime(2015,4,15,0,0,0)\n# end_day = datetime(2015,4,17,0,0,0)\n\nrequest = Request(\n    collection=collection,\n    spatial=BBox(-140, 20, -100, 50), # [20-50N], [-140W, -100W] CA Current\n    #variables=[],\n    # temporal={\n    #     'start': start_day,\n    #     'stop': end_day # goal: try up to 21 days at least,\n    #},\n    granule_id=granuleIDs,\n)\n\nrequest.is_valid()\n\n\nprint(harmony_client.request_as_curl(request))\njob_id = harmony_client.submit(request)\nprint(f'Job ID: {job_id}')\n\n\nharmony_client.status(job_id)\n\n\n# results = harmony_client.result_urls(job_id, link_type=LinkType.s3)\n# urls = list(results)\n# print(urls)\n\n\n# create a new folder to put the subsetted data in\nos.makedirs(\"swot_ocean\",exist_ok = True)\n\n\nfutures = harmony_client.download_all(job_id, directory='./swot_ocean', overwrite=True)\nfile_names = [f.result() for f in futures]\nsorted(file_names)\n\n\nds = xr.open_mfdataset(sorted(file_names),combine='nested',concat_dim='num_lines')\nds\n\n\n# Plot only a pair of passes at a time\ni_time = np.arange(0,1725*2)+1725*90\n\nfig = plt.figure(figsize=[11,7]) \nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\nax.set_extent([-150, -90, 10, 60])\nplt.scatter(ds.longitude[i_time,:], ds.latitude[i_time,:], lw=1, c=ds.ssha_karin[i_time,:])\nplt.colorbar(label='SSHA (m)')\nplt.clim(-1,1)\nplt.show()"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\n\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n\n\n\n\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()\n\n\n\n\n\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html",
    "href": "how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access a single netCDF file from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data\n\n\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport s3fs\nfrom osgeo import gdal\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req\n\n\n\n\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'])\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc'\n\n\n\n\nOpen with the netCDF file using the s3fs package, then load the cloud asset into an xarray dataset.\n\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\n\n\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (time: 1, latitude: 360, longitude: 720, nv: 2)\nCoordinates:\n  * time            (time) datetime64[ns] 2015-01-16T12:00:00\n  * latitude        (latitude) float32 -89.75 -89.25 -88.75 ... 89.25 89.75\n  * longitude       (longitude) float32 -179.8 -179.2 -178.8 ... 179.2 179.8\n    time_bnds       (time, nv) datetime64[ns] 2015-01-01 2015-02-01\n    latitude_bnds   (latitude, nv) float32 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n    longitude_bnds  (longitude, nv) float32 -180.0 -179.5 -179.5 ... 179.5 180.0\nDimensions without coordinates: nv\nData variables:\n    SSH             (time, latitude, longitude) float32 ...\n    SSHIBC          (time, latitude, longitude) float32 ...\n    SSHNOIBC        (time, latitude, longitude) float32 ...\nAttributes: (12/57)\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    Conventions:                  CF-1.8, ACDD-1.3\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2015-02-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2015-01-01T00:00:00\n    title:                        ECCO Sea Surface Height - Monthly Mean 0.5 ...\n    uuid:                         088d03b8-4158-11eb-876b-0cc47a3f47f1xarray.DatasetDimensions:time: 1latitude: 360longitude: 720nv: 2Coordinates: (6)time(time)datetime64[ns]2015-01-16T12:00:00axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000'], dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :Start and end times of averaging period.coverage_content_type :coordinatelong_name :time bounds of averaging periodarray([['2015-01-01T00:00:00.000000000', '2015-02-01T00:00:00.000000000']],\n      dtype='datetime64[ns]')latitude_bnds(latitude, nv)float32...coverage_content_type :coordinatelong_name :latitude bounds grid cellsarray([[-90. , -89.5],\n       [-89.5, -89. ],\n       [-89. , -88.5],\n       ...,\n       [ 88.5,  89. ],\n       [ 89. ,  89.5],\n       [ 89.5,  90. ]], dtype=float32)longitude_bnds(longitude, nv)float32...coverage_content_type :coordinatelong_name :longitude bounds grid cellsarray([[-180. , -179.5],\n       [-179.5, -179. ],\n       [-179. , -178.5],\n       ...,\n       [ 178.5,  179. ],\n       [ 179. ,  179.5],\n       [ 179.5,  180. ]], dtype=float32)Data variables: (3)SSH(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866[259200 values with dtype=float32]SSHIBC(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :The inverted barometer (IB) correction to sea surface height due to atmospheric pressure loadingunits :mcomment :Not an SSH itself, but a correction to model sea level anomaly (ETAN) required to account for the static part of sea surface displacement by atmosphere pressure loading: SSH = SSHNOIBC - SSHIBC. Note: Use SSH for model-data comparisons with altimetry data products that DO apply the IB correction and SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-0.30144819617271423valid_max :0.5245633721351624[259200 values with dtype=float32]SSHNOIBC(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :Sea surface height anomaly without the inverted barometer (IB) correctionunits :mcomment :Sea surface height anomaly above the geoid without the inverse barometer (IB) correction, suitable for comparisons with altimetry sea surface height data products that do NOT apply the inverse barometer (IB) correction. Note: SSHNOIBC is calculated by correcting model sea level anomaly ETAN for two effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). In ECCO Version 4 Release 4 the model is forced with atmospheric pressure loading. SSHNOIBC does not correct for the static part of the effect of atmosphere pressure loading on sea surface height (the so-called inverse barometer (IB) correction). Use SSH for comparisons with altimetry data products that DO apply the IB correction.valid_min :-1.6654272079467773valid_max :1.4550364017486572[259200 values with dtype=float32]Attributes: (57)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on a regular lat-lon grid. They have been mapped to the regular lat-lon grid from the original ECCO lat-lon-cap 90 (llc90) native model grid. SSH (dynamic sea surface height) = SSHNOIBC (dynamic sea surface without the inverse barometer correction) - SSHIBC (inverse barometer correction). The inverted barometer correction accounts for variations in sea surface height due to atmospheric pressure variations.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T09:39:51date_issued :2020-12-18T09:39:51date_metadata_modified :2021-03-15T22:07:49date_modified :2021-03-15T22:07:49geospatial_bounds_crs :EPSG:4326geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.5geospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :0.5geospatial_lon_units :degrees_easthistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECG5M-SSH44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE > OCEANS > SEA SURFACE TOPOGRAPHY > SEA SURFACE HEIGHT, EARTH SCIENCE SERVICES > MODELS > EARTH SCIENCE REANALYSES/ASSIMILATION MODELSkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_SSH_05DEG_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged dynamic sea surface height interpolated to a regular 0.5-degree grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2015-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2015-01-01T00:00:00title :ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)uuid :088d03b8-4158-11eb-876b-0cc47a3f47f1\n\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'SSH' (time: 1, latitude: 360, longitude: 720)>\n[259200 values with dtype=float32]\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              -1.8805772066116333\n    valid_max:              1.4207719564437866xarray.DataArray'SSH'time: 1latitude: 360longitude: 720...[259200 values with dtype=float32]Coordinates: (3)time(time)datetime64[ns]2015-01-16T12:00:00axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000'], dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)Attributes: (7)coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866\n\n\nPlot the SSH dataarray for time 2015-01-16T12:00:00 using hvplot.\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html",
    "href": "how-tos/Earthdata_Cloud__Open-Science-Tutorial.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "Each of the following co-authors contributed to the following materials and code examples, as well as collaboration infrastructure for the NASA Earthdata Openscapes Project: * Julia S. Stewart Lowndes; Openscapes, NCEAS * Erin Robinson; Openscapes, Metadata Game Changers * Catalina M Oaida; NASA PO.DAAC, NASA Jet Propulsion Laboratory * Luis Alberto Lopez; NASA National Snow and Ice Data Center DAAC * Aaron Friesz; NASA Land Processes DAAC * Andrew P Barrett; NASA National Snow and Ice Data Center DAAC * Makhan Virdi; NASA ASDC DAAC * Jack McNelis; NASA PO.DAAC, NASA Jet Propulsion Laboratory\nAdditional credit to the entire NASA Earthdata Openscapes Project community, Patrick Quinn at Element84, and to2i2c for our Cloud computing infrastructure\n\n\n\n\nIntroduction to NASA Earthdata’s move to the cloud\n\nBackground and motivation\nEnabling Open Science via “Analysis-in-Place”\nResources for cloud adopters: NASA Earthdata Openscapes\n\nNASA Earthdata discovery and access in the cloud\n\nPart 1: Explore Earthdata cloud data availablity\nPart 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nPart 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\n\n\n\n\n\nThis notebook source code: update https://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/how-tos\nAlso available via online Quarto book: update https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/\n\n\n\n\n\n\n\n\nEOSDIS Data Archive\n\n\n\n\n\n \n\nNASA Distributed Active Archive Centers (DAACs) are continuing to migrate data to the Earthdata Cloud\n\nSupporting increased data volume as new, high-resolution remote sensing missions launch in the coming years\nData hosted via Amazon Web Services, or AWS\nDAACs continuing to support tools, services, and tutorial resources for our user communities\n\n\n\n\n\n\nReducing barriers to large-scale scientific research in the era of “big data”\nIncreasing community contributions with hands-on engagement\nPromoting reproducible and shareable workflows without relying on local storage systems\n\n\n\n\nOpen Data\n\n\n\n\n\n\n\n\nEarthdata Cloud Paradigm\n\n\n\n\n\nShow slide with 3 panels of user resources\nEmphasize that the following tutorials are short examples that were taken from the tutorial resources we have been building for our users\n\n\n\nThe following tutorial demonstrates several basic end-to-end workflows to interact with data “in-place” from the NASA Earthdata Cloud, accessing Amazon Web Services (AWS) Single Storage Solution (S3) data locations without the need to download data. While the data can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow.\nAlthough the examples we’re working with in this notebook only focuses on a small time and area for demonstration purposes, this workflow can be modified and scaled up to suit a larger time range and region of interest.\n\n\n\nHarmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002)\n\nSurface reflectance (SR) and top of atmosphere (TOA) brightness data\nGlobal observations of the land every 2–3 days at 30-meter (m)\nCloud Optimized GeoTIFF (COG) format\n\nECCO Sea Surface Height - Daily Mean 0.5 Degree (Version 4 Release 4)(10.5067/ECG5D-SSH44).\n\nDaily-averaged dynamic sea surface height\nTime series of monthly NetCDFs on a 0.5-degree latitude/longitude grid.\n\n\n\n\n\n\n\n\nFrom Earthdata Search https://search.earthdata.nasa.gov, use your Earthdata login credentials to log in. You can create an Earthdata Login account at https://urs.earthdata.nasa.gov.\nIn this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 39 matching collections were found with the ECCO monthly SSH search, and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box. Scroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\n\n\n\nClicking on the ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4) dataset, we now see a list of files (granules) that are part of the dataset (collection). We can click on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\n\n\n\n\nSelect the “Direct Download” option to view Access options via Direct Download and from the AWS Cloud. Additional options to customize the data are also available for this dataset.\n\n\n\nFigure caption: Customize your download or access\n\n\n\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. The AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud.\nE.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\n\n\nIn the next two examples we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\n\n\n\nFigure caption: Direct S3 access\n\n\n\n\n\n\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format archived by the Land Processes (LP) DAAC. The COGs can be used like any other GeoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling.\n\n\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\n\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\n\n\n\n\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this example, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range.\n\n\n\n\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nprovider_cat = Client.open(STAC_URL)\n\n\n\nFor this next step we need the provider title (e.g., LPCLOUD). We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects.\n\n\n\n\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. We’ll also specify the data collections and a time range for our example.\n\n\nReading in a geojson file with geopandas and extract coodinates for our ROI.\n\nfield = geopandas.read_file('../data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]\nroi = json.loads(field.to_json())['features'][0]['geometry']\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API.\n\n\n\n\nNow we can put all our search criteria together using catalog.search from the pystac_client package. STAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\n\ndate_range = \"2021-05/2021-08\"\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\n\n\nprint('Matching STAC Items:', search.matched())\nitem_collection = search.get_all_items()\nitem_collection[0].to_dict()\n\nMatching STAC Items: 113\n\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T13TGF.2021124T173013.v2.0',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 36},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-101.5423534, 40.5109845],\n    [-101.3056118, 41.2066375],\n    [-101.2894253, 41.4919436],\n    [-102.6032964, 41.5268623],\n    [-102.638891, 40.5386175],\n    [-101.5423534, 40.5109845]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T13TGF.2021124T173013.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.umm_json'}],\n 'assets': {'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.638891, 40.510984, -101.289425, 41.526862],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}\n\n\n\n\n\n\nBelow we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis. We will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above) and print out the first ten links, converted to s3 locations:\n\ncloudcover = 25\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n                \ns3_links = [l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://') for l in evi_band_links]\ns3_links[:10]\n\n['s3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B8A.tif']\n\n\n\n\n\nAccess s3 credentials from LP.DAAC and create a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\ntemp_creds_req = requests.get(s3_cred_endpoint).json()\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL Configurations\nGDAL is a foundational piece of geospatial software that is leveraged by several popular open-source, and closed, geospatial software. The rasterio package is no exception. Rasterio leverages GDAL to, among other things, read and write raster data files, e.g., GeoTIFFs/Cloud Optimized GeoTIFFs. To read remote files, i.e., files/objects stored in the cloud, GDAL uses its Virtual File System API. In a perfect world, one would be able to point a Virtual File System (there are several) at a remote data asset and have the asset retrieved, but that is not always the case. GDAL has a host of configurations/environmental variables that adjust its behavior to, for example, make a request more performant or to pass AWS credentials to the distribution system. Below, we’ll identify the evironmental variables that will help us get our data from cloud\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f64510812e0>\n\n\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n# s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif'\n\n\n\n\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\n\n\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nrio_env.__exit__()\n\n\n\n\nWe have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3). In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations.\nThis example demonstrates “analysis in place” using customized ECCO Level 4 monthly sea surface height data, in this case reformatted to Zarr, from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony:\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow us to request only the data we want, in the format and projection we want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge\n\n\n\n\n\nfrom harmony import BBox, Client, Collection, Request, LinkType\nfrom harmony.config import Environment\nfrom pprint import pprint\nimport datetime as dt\nimport s3fs\nfrom pqdm.threads import pqdm\nimport xarray as xr\n\n\n\n\nHarmony-Py provides a pip installable Python alternative to directly using Harmony’s RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\n\n\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\n\nharmony_client = Client()\n\n\n\n\n\nSpecify a temporal range over 2015, and Zarr as an output format.\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\nrequest = Request(\n    collection=Collection(id=short_name),\n    temporal={\n        'start': dt.datetime(2015, 1, 2),\n        'stop': dt.datetime(2015, 12, 31),\n    },\n    format='application/x-zarr'\n)\n\njob_id = harmony_client.submit(request)\n\n\n\n\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response:\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\nresults = harmony_client.result_urls(job_id, link_type=LinkType.s3)\ns3_urls = list(results)\ns3_urls\n\n [ Processing:  83% ] |##########################################         | [/]\n\n\n\n\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\ncreds = harmony_client.aws_credentials()\n\n\n\n\n\nAccess AWS credentials for the Harmony bucket, and use the AWS s3fs package to create a file system that can then be read by xarray. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\ncreds = harmony_client.aws_credentials()\n\ns3_fs = s3fs.S3FileSystem(\n    key=creds['aws_access_key_id'],\n    secret=creds['aws_secret_access_key'],\n    token=creds['aws_session_token'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nOpen the Zarr stores using the s3fs package, then load them all at once into a concatenated xarray dataset:\n\nstores = [s3fs.S3Map(root=url, s3=s3_fs, check=False) for url in s3_urls]\ndef open_zarr_xarray(store):\n    return xr.open_zarr(store=store, consolidated=True)\n\ndatasets = pqdm(stores, open_zarr_xarray, n_jobs=12)\n\nds = xr.concat(datasets, 'time', coords='minimal', )\nds = xr.decode_cf(ds, mask_and_scale=True, decode_coords=True)\nds\n\n\nssh_da = ds.SSH\n\nssh_da.to_masked_array(copy=False)\n\nssh_da\n\n\n\n\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\nssh_da = ds.SSH\n\nstdev_ssh = ssh_da.std('time')\nstdev_ssh.name = 'stdev of analysed_sst [Kelvin]'\nstdev_ssh.plot();\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))\n\n\n\n\n\nReference Hackathon/workshop tutorials that go into more detail!\nEarthdata Cloud Cookbook\nEarthdata Cloud Primer\n\nGetting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment."
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html",
    "href": "how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to configure you Python work environment to access Cloud Optimized geoTIFF (COG) files\nhow to access HLS COG files\nhow to plot the data\n\n\n\n\n\nUsing Harmonized Landsat Sentinel-2 (HLS) version 2.0\n\n\n\nimport os\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n\n\n\n\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(https_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html",
    "href": "how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into an xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to define a dataset of interest and find netCDF files in S3 bucket\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data\n\n\n\n\n\n\nimport os\nimport requests\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req\n\n\n\n\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'],\n                          client_kwargs={'region_name':'us-west-2'})\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data (ECCO_L4_SSH_05DEG_MONTHLY_V4R4).\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\n\nbucket = os.path.join('podaac-ops-cumulus-protected/', short_name, '*2015*.nc')\nbucket\n\n'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/*2015*.nc'\n\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_files = fs_s3.glob(bucket)\nssh_files\n\n['podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-02_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-03_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-04_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-05_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-06_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-07_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-08_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-10_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-11_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-12_ECCO_V4r4_latlon_0p50deg.nc']\n\n\n\n\n\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nfileset = [fs_s3.open(file) for file in ssh_files]\n\nCreate an xarray dataset using the open_mfdataset() function to “read in” all of the netCDF4 files in one call.\n\nssh_ds = xr.open_mfdataset(fileset,\n                           combine='by_coords',\n                           mask_and_scale=True,\n                           decode_cf=True,\n                           chunks='auto')\nssh_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (time: 12, latitude: 360, longitude: 720, nv: 2)\nCoordinates:\n  * time            (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T...\n  * latitude        (latitude) float32 -89.75 -89.25 -88.75 ... 89.25 89.75\n  * longitude       (longitude) float32 -179.8 -179.2 -178.8 ... 179.2 179.8\n    time_bnds       (time, nv) datetime64[ns] dask.array<chunksize=(1, 2), meta=np.ndarray>\n    latitude_bnds   (latitude, nv) float32 dask.array<chunksize=(360, 2), meta=np.ndarray>\n    longitude_bnds  (longitude, nv) float32 dask.array<chunksize=(720, 2), meta=np.ndarray>\nDimensions without coordinates: nv\nData variables:\n    SSH             (time, latitude, longitude) float32 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    SSHIBC          (time, latitude, longitude) float32 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    SSHNOIBC        (time, latitude, longitude) float32 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\nAttributes: (12/57)\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    Conventions:                  CF-1.8, ACDD-1.3\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2015-02-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2015-01-01T00:00:00\n    title:                        ECCO Sea Surface Height - Monthly Mean 0.5 ...\n    uuid:                         088d03b8-4158-11eb-876b-0cc47a3f47f1xarray.DatasetDimensions:time: 12latitude: 360longitude: 720nv: 2Coordinates: (6)time(time)datetime64[ns]2015-01-16T12:00:00 ... 2015-12-...axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', '2015-04-16T00:00:00.000000000',\n       '2015-05-16T12:00:00.000000000', '2015-06-16T00:00:00.000000000',\n       '2015-07-16T12:00:00.000000000', '2015-08-16T12:00:00.000000000',\n       '2015-09-16T00:00:00.000000000', '2015-10-16T12:00:00.000000000',\n       '2015-11-16T00:00:00.000000000', '2015-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)time_bnds(time, nv)datetime64[ns]dask.array<chunksize=(1, 2), meta=np.ndarray>comment :Start and end times of averaging period.coverage_content_type :coordinatelong_name :time bounds of averaging period\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         192 B \n                         16 B \n                    \n                    \n                    \n                         Shape \n                         (12, 2) \n                         (1, 2) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     datetime64[ns] \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  12\n\n        \n    \nlatitude_bnds(latitude, nv)float32dask.array<chunksize=(360, 2), meta=np.ndarray>coverage_content_type :coordinatelong_name :latitude bounds grid cells\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         2.81 kiB \n                         2.81 kiB \n                    \n                    \n                    \n                         Shape \n                         (360, 2) \n                         (360, 2) \n                    \n                    \n                         Count \n                         55 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  360\n\n        \n    \nlongitude_bnds(longitude, nv)float32dask.array<chunksize=(720, 2), meta=np.ndarray>coverage_content_type :coordinatelong_name :longitude bounds grid cells\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         5.62 kiB \n                         5.62 kiB \n                    \n                    \n                    \n                         Shape \n                         (720, 2) \n                         (720, 2) \n                    \n                    \n                         Count \n                         55 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  720\n\n        \n    \nData variables: (3)SSH(time, latitude, longitude)float32dask.array<chunksize=(1, 360, 720), meta=np.ndarray>coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nSSHIBC(time, latitude, longitude)float32dask.array<chunksize=(1, 360, 720), meta=np.ndarray>coverage_content_type :modelResultlong_name :The inverted barometer (IB) correction to sea surface height due to atmospheric pressure loadingunits :mcomment :Not an SSH itself, but a correction to model sea level anomaly (ETAN) required to account for the static part of sea surface displacement by atmosphere pressure loading: SSH = SSHNOIBC - SSHIBC. Note: Use SSH for model-data comparisons with altimetry data products that DO apply the IB correction and SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-0.30144819617271423valid_max :0.5245633721351624\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nSSHNOIBC(time, latitude, longitude)float32dask.array<chunksize=(1, 360, 720), meta=np.ndarray>coverage_content_type :modelResultlong_name :Sea surface height anomaly without the inverted barometer (IB) correctionunits :mcomment :Sea surface height anomaly above the geoid without the inverse barometer (IB) correction, suitable for comparisons with altimetry sea surface height data products that do NOT apply the inverse barometer (IB) correction. Note: SSHNOIBC is calculated by correcting model sea level anomaly ETAN for two effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). In ECCO Version 4 Release 4 the model is forced with atmospheric pressure loading. SSHNOIBC does not correct for the static part of the effect of atmosphere pressure loading on sea surface height (the so-called inverse barometer (IB) correction). Use SSH for comparisons with altimetry data products that DO apply the IB correction.valid_min :-1.6654272079467773valid_max :1.4550364017486572\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nAttributes: (57)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on a regular lat-lon grid. They have been mapped to the regular lat-lon grid from the original ECCO lat-lon-cap 90 (llc90) native model grid. SSH (dynamic sea surface height) = SSHNOIBC (dynamic sea surface without the inverse barometer correction) - SSHIBC (inverse barometer correction). The inverted barometer correction accounts for variations in sea surface height due to atmospheric pressure variations.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T09:39:51date_issued :2020-12-18T09:39:51date_metadata_modified :2021-03-15T22:07:49date_modified :2021-03-15T22:07:49geospatial_bounds_crs :EPSG:4326geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.5geospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :0.5geospatial_lon_units :degrees_easthistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECG5M-SSH44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE > OCEANS > SEA SURFACE TOPOGRAPHY > SEA SURFACE HEIGHT, EARTH SCIENCE SERVICES > MODELS > EARTH SCIENCE REANALYSES/ASSIMILATION MODELSkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_SSH_05DEG_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged dynamic sea surface height interpolated to a regular 0.5-degree grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2015-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2015-01-01T00:00:00title :ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)uuid :088d03b8-4158-11eb-876b-0cc47a3f47f1\n\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 360, 720), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              -1.8805772066116333\n    valid_max:              1.4207719564437866xarray.DataArray'SSH'time: 12latitude: 360longitude: 720dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nCoordinates: (3)time(time)datetime64[ns]2015-01-16T12:00:00 ... 2015-12-...axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', '2015-04-16T00:00:00.000000000',\n       '2015-05-16T12:00:00.000000000', '2015-06-16T00:00:00.000000000',\n       '2015-07-16T12:00:00.000000000', '2015-08-16T12:00:00.000000000',\n       '2015-09-16T00:00:00.000000000', '2015-10-16T12:00:00.000000000',\n       '2015-11-16T00:00:00.000000000', '2015-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)Attributes: (7)coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866\n\n\nPlot the SSH time series using hvplot\n\nssh_da.hvplot.image(y='latitude', x='longitude', cmap='Viridis',).opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/Multi-File_Direct_S3_Access_COG_Example.html",
    "href": "how-tos/Multi-File_Direct_S3_Access_COG_Example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\ncatalog = Client.open(f\"{STAC_URL}/LPCLOUD\")\n\n\nsearch = catalog.search(\n    collections = ['HLSL30.v2.0', 'HLSS30.v2.0'],\n    intersects = {'type': 'Polygon',\n                  'coordinates': [[[-101.67271614074707, 41.04754380304359],\n                                   [-101.65344715118408, 41.04754380304359],\n                                   [-101.65344715118408, 41.06213891056728],\n                                   [-101.67271614074707, 41.06213891056728],\n                                   [-101.67271614074707, 41.04754380304359]]]},\n    datetime = '2021-05/2021-08'\n)               \n\n\nsearch.matched()\n\n\nic = search.get_all_items()\n\n\nil = list(search.get_items())\n\n\ntic = [x for x in ic if 'T13TGF' in x.id]\n\n\nimport pystac\n\n\nitem_collection = pystac.ItemCollection(items=tic)\n\n\nitem_collection\n\n\nil\n\n\ndata = stackstac.stack(item_collection, assets=['B04', 'B02'], epsg=32613, resolution=30)\n\n\ndata.sel(band='B04').isel(time=[0])\n\n\nimport stackstac\nimport pystac_client\n\nURL = \"https://earth-search.aws.element84.com/v0\"\ncatalog = pystac_client.Client.open(URL)\n\n\ncatalog\n\n\nstac_items = catalog.search(\n    intersects=dict(type=\"Point\", coordinates=[-105.78, 35.79]),\n    collections=[\"sentinel-s2-l2a-cogs\"],\n    datetime=\"2020-04-01/2020-05-01\"\n).get_all_items()\n\n\nstac_items\n\n\nstack = stackstac.stack(stac_items)\n\n\nstack"
  },
  {
    "objectID": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html",
    "href": "how-tos/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html",
    "title": "2022 SWOT Oceanography Cloud Workshop",
    "section": "",
    "text": "Requirements Earthdata Login\n\nOPeNDAP (Hyrax Server)\n\nOn-prem Endpoint - Open\nOn-prem Endpoint - Earthdata Login Authentication\nEarthdata Cloud Endpoint - Earthdata Login Authentication\n\n\n\n\n\n\nimport xarray as xr\nimport dask\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nopd_sst_url = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/NCEI/AVHRR_OI/v2/1981/244/19810901120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.0.nc'\n\n\nopd_sst_ds = xr.open_dataset(opd_sst_url)\nopd_sst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (lat: 720, lon: 1440, time: 1, nv: 2)\nCoordinates:\n  * lat               (lat) float32 -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon               (lon) float32 -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time              (time) datetime64[ns] 1981-09-01\nDimensions without coordinates: nv\nData variables:\n    lat_bnds          (lat, nv) float32 -90.0 -89.75 -89.75 ... 89.75 89.75 90.0\n    lon_bnds          (lon, nv) float32 -180.0 -179.8 -179.8 ... 179.8 180.0\n    time_bnds         (time, nv) datetime64[ns] 1981-09-01 1981-09-02\n    analysed_sst      (time, lat, lon) float32 ...\n    analysis_error    (time, lat, lon) float32 ...\n    mask              (time, lat, lon) float32 ...\n    sea_ice_fraction  (time, lat, lon) float32 ...\nAttributes: (12/48)\n    product_version:                 Version 2.0\n    spatial_resolution:              0.25 degree\n    Conventions:                     CF-1.6,ACDD-1.3\n    title:                           NCEI global 0.25 deg daily sea surface t...\n    references:                      Reynolds, et al.(2009) What is New in Ve...\n    institution:                     NCEI\n    ...                              ...\n    source:                          AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SH...\n    summary:                         NOAA's 1/4-degree Daily Optimum Interpol...\n    time_coverage_start:             19810901T000000Z\n    time_coverage_end:               19810902T000000Z\n    uuid:                            39832cc3-d409-438a-820e-2bb1b38ebca8\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lat: 720lon: 1440time: 1nv: 2Coordinates: (3)lat(lat)float32-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northcomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degrees.bounds :lat_bndsvalid_max :90.0valid_min :-90.0array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float32-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastcomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degrees.bounds :lon_bndsvalid_max :180.0valid_min :-180.0array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)time(time)datetime64[ns]1981-09-01long_name :reference time of sst fieldstandard_name :timeaxis :Tbounds :time_bndscomment :Nominal time because observations are from different sources and are made at different times of the day.array(['1981-09-01T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (7)lat_bnds(lat, nv)float32...comment :This variable defines the latitude values at the north and south bounds of every 0.25-degree pixel.array([[-90.  , -89.75],\n       [-89.75, -89.5 ],\n       [-89.5 , -89.25],\n       ...,\n       [ 89.25,  89.5 ],\n       [ 89.5 ,  89.75],\n       [ 89.75,  90.  ]], dtype=float32)lon_bnds(lon, nv)float32...comment :This variable defines the longitude values at the west and east bounds of every 0.25-degree pixel.array([[-180.  , -179.75],\n       [-179.75, -179.5 ],\n       [-179.5 , -179.25],\n       ...,\n       [ 179.25,  179.5 ],\n       [ 179.5 ,  179.75],\n       [ 179.75,  180.  ]], dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :This variable defines the start and end of the time span for the data.array([['1981-09-01T00:00:00.000000000', '1981-09-02T00:00:00.000000000']],\n      dtype='datetime64[ns]')analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500comment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are 'near real time' quality for recent period.  SST (bulk) is at ambiguous depth because multiple types of observations are used.source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]analysis_error(time, lat, lon)float32...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :Sum of bias, sampling and random errors.[1036800 values with dtype=float32]mask(time, lat, lon)float32...long_name :sea/land field composite maskflag_meanings :water landcomment :Binary mask distinguishing water and land only.flag_masks :[1 2]source :RWReynolds_landmask_V1.0valid_max :2valid_min :1[1036800 values with dtype=float32]sea_ice_fraction(time, lat, lon)float32...long_name :sea ice area fractionvalid_min :0valid_max :100standard_name :sea_ice_area_fractionunits :1comment :7-day median filtered .  Switch from 25 km NASA team ice (http://nsidc.org/data/nsidc-0051.html)  to 50 km NCEP ice (http://polar.ncep.noaa.gov/seaice) after 2004 results in artificial increase in ice coverage.source :GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]Attributes: (48)product_version :Version 2.0spatial_resolution :0.25 degreeConventions :CF-1.6,ACDD-1.3title :NCEI global 0.25 deg daily sea surface temperature analysis based mainly on Advanced Very High Resolution Radiometer, finalreferences :Reynolds, et al.(2009) What is New in Version 2. Available at http://www.ncdc.noaa.gov/sites/default/files/attachments/Reynolds2009_oisst_daily_v02r00_version2-features.pdf; Daily 1/4 Degree Optimum Interpolation Sea Surface Temperature (OISST)- Climate Algorithm Theoretical Theoretical Basis Document, NOAA Climate Data Record Program CDRP-ATBD-0303 Rev. 2 (2013). Available at http://www1.ncdc.noaa.gov/pub/data/sds/cdr/CDRs/Sea_Surface_Temperature_Optimum_Interpolation/AlgorithmDescription.pdf.institution :NCEInetcdf_version_id :4.3.2history :2015-11-02T19:52:40Z: Modified format and attributes with NCO to match the GDS 2.0 rev 5 specification.start_time :19810901T000000Zstop_time :19810902T000000Zwesternmost_longitude :-180.0easternmost_longitude :180.0southernmost_latitude :-90.0northernmost_latitude :90.0comment :The daily OISST version 2.0 data contained in this file are the same as those in the equivalent GDS 1.0 file.Metadata_Conventions :ACDD-1.3acknowledgment :This project was supported in part by a grant from the NOAA Climate Data Record (CDR) Program. Cite this dataset when used as a source. The recommended citation and DOI depends on the data center from which the files were acquired. For data accessed from NOAA in near real-time or from the GHRSST LTSRF, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. NOAA National Centers for Environmental Information. http://doi.org/doi:10.7289/V5SQ8XB5 [access date]. For data accessed from the NASA PO.DAAC, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. PO.DAAC, CA, USA. http://doi.org/10.5067/GHAAO-4BC01 [access date].cdm_data_type :Gridcreator_name :Viva Banzoncreator_email :viva.banzon@noaa.govcreator_url :http://www.ncdc.noaa.govdate_created :20091203T000000Zfile_quality_level :3gds_version_id :2.0r5geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastid :NCEI-L4LRblend-GLOB-AVHRR_OIkeywords :Oceans>Ocean Temperature>Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywords, Version 8.1license :No constraints on data access or use.metadata_link :http://doi.org/10.7289/V5SQ8XB5naming_authority :org.ghrsstplatform :NOAA-7processing_level :L4project :Group for High Resolution Sea Surface Temperaturepublisher_email :oisst_contacts@noaa.govpublisher_name :OISST Operations Teampublisher_url :http://www.ncdc.noaa.gov/sstsensor :AVHRR_GACstandard_name_vocabulary :CF Standard Name Table v29source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICEsummary :NOAA's 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynold's SST, which however also refers to earlier products at different resolution), currently available as version 2,  is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002.time_coverage_start :19810901T000000Ztime_coverage_end :19810902T000000Zuuid :39832cc3-d409-438a-820e-2bb1b38ebca8DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nopd_sst_ds.analysed_sst.isel(time=0).hvplot.image(cmap='Inferno')\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimport opendap_auth\n\n\nopendap_auth.create_dodsrc()\n\n'.dodsrc file created: /home/jovyan/.dodsrc'\n\n\nIntegrated Multi-satellitE Retrievals for GPM (IMERG) Level 3 IMERG Final Daily 10 x 10 km (GPM_3IMERGDF)\n\nopd_prec_url = 'https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGDF.06/2021/07/3B-DAY.MS.MRG.3IMERG.20210704-S000000-E235959.V06.nc4' \n\n\nopd_prec_ds = xr.open_dataset(opd_prec_url)\nopd_prec_ds\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\nKeyboardInterrupt: \n\n\n\nopd_prec_ds.precipitationCal.isel(time=0).hvplot.image(cmap='rainbow')\n\n\n\n\n\nedc_odp_ssh_url = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc'\n\n\nedc_odp_ssh_ds = xr.open_dataset(edc_odp_ssh_url)\nedc_odp_ssh_ds\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/GHRSST%20Level%204%20MUR%20Global%20Foundation%20Sea%20Surface%20Temperature%20Analysis%20(v4.1)/granules/20190201090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.dap.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc4'\n\n\nxr.open_dataset(url)"
  }
]